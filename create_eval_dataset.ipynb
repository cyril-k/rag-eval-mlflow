{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of custom RAG evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "TEMPERATURE=0.1\n",
    "TOP_P=0.95\n",
    "MAX_TOKENS=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"meta-llama/Meta-Llama-3.1-405B-Instruct\"\n",
    "\n",
    "llm_client = ChatOpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NB_AI_STUDIO_KEY\"),\n",
    "    model=MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    top_p=TOP_P,\n",
    "    max_tokens=MAX_TOKENS,\n",
    ")\n",
    "\n",
    "llm_client.invoke(\"What is the capital of France\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate QA pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate RAG ppeline, we need to have areference dataset which will include golden QA pairs together with reference context. \n",
    "\n",
    "\n",
    "To generate QA pairs we are going to use `jamescalam/ai-arxiv-chunked` which contains chunkd of NLP-related reseearch papers. We are not going to use the chunks themselves but only paper summaries which are provided together with chunks. \n",
    "There are over 400 unique summaries and we are going to sample 100 of them to use as source context for QA pairs generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "summaries = list(set(data[\"summary\"]))\n",
    "sampled_summaries = random.sample(summaries, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make batches of 4 to accelerate the generation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "batches = [sampled_summaries[i * BATCH_SIZE:(i+1) * BATCH_SIZE] for i in range(len(sampled_summaries)//BATCH_SIZE)]\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create some helper function which will process batches of summaries and generate QA pairs. It is usefull to wrap this function with `retry` in case we run into rate limit for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Your task is to write a standalone factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "Your answer to the factoid question should be detailed and relying upon given context and be accessible for a wide variety of users.\n",
    "This means that your standalone factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "gen_prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\", USER_PROMPT),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_prompt_template.batch(inputs=[{\"context\": \"context\"},{\"context\": \"context\"}])\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=4, max=120), stop=stop_after_attempt(3))\n",
    "def process_batch_messages(llm, batch):\n",
    "    return llm.batch(gen_prompt_template.batch(inputs=[{\"context\": d} for d in batch]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other helper functions which may be useful later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def hash_string(input: str) -> str:\n",
    "    h = hashlib.new('sha256')\n",
    "    h.update(input.encode())\n",
    "    \n",
    "    return h.hexdigest()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_timestamp() -> str:\n",
    "    timestamp = datetime.now()\n",
    "    return timestamp.strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code which processes the batches and parses the output into QA pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/09/25 14:43:38 INFO mlflow.tracking.fluent: Experiment with name 'Data generation for RAG eval 2024-09-25_14-43-38' does not exist. Creating a new experiment.\n",
      "2024/09/25 14:43:39 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of langchain. If you encounter errors during autologging, try upgrading / downgrading langchain to a supported version, or try upgrading MLflow.\n",
      "100%|██████████| 25/25 [00:32<00:00,  1.28s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import mlflow\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "formatted_timestamp = get_timestamp()\n",
    "mlflow.set_experiment(f\"Data generation for RAG eval {formatted_timestamp}\")\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "file_path = f'generated_items_{formatted_timestamp}.jsonl'\n",
    "\n",
    "outputs = []\n",
    "for batch in tqdm(batches[:]):\n",
    "    responses = process_batch_messages(llm_client, batch)\n",
    "    for response, doc in zip(responses, batch):\n",
    "        output_QA_couple = response.content\n",
    "        try:\n",
    "            question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "            answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "            item =  {\n",
    "                \"document\": {\n",
    "                    \"content\": doc,\n",
    "                    \"collection_id\": str(hash_string(doc)),\n",
    "                },\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "            }\n",
    "            outputs.append(item)\n",
    "            json_line = json.dumps(item)\n",
    "            with open(file_path, 'a') as file:\n",
    "                file.write(json_line + '\\n')\n",
    "        except Exception as e:\n",
    "            print(e.__str__())\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Evaluate generated eval dataset with Prometheus 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start serving `text-generation-inference` with `prometheus-bgb-8x7b-v2.0` LLM:\n",
    "\n",
    "```text\n",
    "docker run -e HF_TOKEN=$HF_TOKEN --gpus all --shm-size 64g -p 8089:80 -v $PWD/data:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:2.2.0 \\\n",
    "    --model-id prometheus-eval/prometheus-bgb-8x7b-v2.0 \\\n",
    "    --dtype bfloat16 \\\n",
    "    --sharded true \\\n",
    "    --num-shard 2 \\\n",
    "    --max-input-tokens 32000 \\\n",
    "    --max-total-tokens 32768\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a custom `LiteLLM` client which will be using our self-hosted model on TGI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_eval.litellm import LiteLLM\n",
    "from prometheus_eval import PrometheusEval\n",
    "from prometheus_eval.prompts import SCORE_RUBRIC_TEMPLATE\n",
    "from typing import Tuple\n",
    "\n",
    "_MODEL = \"prometheus-eval/prometheus-bgb-8x7b-v2.0\"\n",
    "\n",
    "class CustomLiteLLM(LiteLLM):\n",
    "    def __init__(self, api_key: str=\"-\", **kwargs):\n",
    "        \"\"\"Initialize the LiteLLM with basic configurations.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.api_key = api_key\n",
    "\n",
    "    def completions(self, *args, **kwargs):\n",
    "\n",
    "        kwargs.update({\"api_key\": self.api_key})\n",
    "        return super().completions(*args, **kwargs)\n",
    "\n",
    "litellm_client = CustomLiteLLM(\n",
    "    name=f\"huggingface/{_MODEL}\",\n",
    "    api_base=\"http://localhost:8089\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necessary pormpts required for evaluation of generated QA pairs with Prometheus-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prometheus_prompt = \"\"\"###Task Description:\n",
    "An instruction (might include an Input inside it), a question to evaluate, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"(write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{instruction}\n",
    "\n",
    "###Question to evaluate:\n",
    "{response}\n",
    "\n",
    "###Score Rubrics:\n",
    "{rubric}\n",
    "\n",
    "###Feedback: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_rubric_data = {\n",
    "    \"criteria\": \"The Question to evaluate is generated by a model in response to an instruction or question and uses the provided Context. Is question formulated clearly, without ambiguity, and remain grounded in the Context?\",\n",
    "    \"score1_description\": \"The question is vague or unclear and does not engage with the provided Context, making it impossible to discern how it relates to the question or instruction.\",\n",
    "    \"score2_description\": \"The question partially engages with the Context but includes significant ambiguities or unclear portions, often straying from the context or not fully addressing the question.\",\n",
    "    \"score3_description\": \"The question generally addresses the question using the provided Context but has occasional ambiguities or is unclear in certain aspects, making parts of the question less grounded.\",\n",
    "    \"score4_description\": \"The question is mostly clear and unambiguous, providing a grounded question based on the Context. However, there are minor instances where clarity could be improved or where the grounding in the context is weaker.\",\n",
    "    \"score5_description\": \"The question is entirely clear, unambiguous, and fully grounded in the provided Context. It aligns with context precisely with no unnecessary or unclear content.\"\n",
    "}\n",
    "\n",
    "relevance_rubric_data = {\n",
    "    \"criteria\": \"The Question to evaluate is intended for NLP researchers and practitioners. How relevant and useful is this question to the practical needs, concerns, or tasks of NLP practitioners?\",\n",
    "    \"score1_description\": \"The question is irrelevant or entirely unrelated to NLP researchers and practitioners. It provides no utility and does not address any relevant issues or tasks.\",\n",
    "    \"score2_description\": \"The question has minimal relevance to research and operations on NLP. It touches on tangential or obscure topics with little practical use for most researchers and practitioners.\",\n",
    "    \"score3_description\": \"The question is somewhat relevant but lacks focus on key NLP use cases or concerns. While it could be useful in some scenarios, it doesn't address a common or critical issue for researchers and practitioners.\",\n",
    "    \"score4_description\": \"The question is relevant and addresses a useful or moderately important aspect of NLP. It could help researchers and practitioners solve a typical problem or address a common task, but it may not target a critical or high-impact area.\",\n",
    "    \"score5_description\": \"The question is highly relevant to NLP researchers and practitioners. It addresses a common, critical, or high-impact issue or task that many users are likely to face, making it very useful for NLP researchers and practitioners.\"\n",
    "}\n",
    "\n",
    "standalone_rubric_data = {\n",
    "    \"criteria\": \"The Question to evaluate is intended to be self-contained. Can the question be understood and answered without looking at the Context in the Instruction to evaluate, or does it depend on external information (like previous context, documents, or scenarios) to be fully comprehensible?\",\n",
    "    \"score1_description\": \"The question heavily depends on external context or previous information to be understood. It refers to specific content (e.g., 'in the context' or 'in the document') and is incomplete on its own.\",\n",
    "    \"score2_description\": \"The question is mostly dependent on external information. While parts of the question may be clear, it still requires knowledge of additional context or documents to be fully understood.\",\n",
    "    \"score3_description\": \"The question is partially understandable on its own but still relies on some implicit context or background knowledge to be fully clear. It is incomplete without certain pieces of information.\",\n",
    "    \"score4_description\": \"The question is largely standalone and makes sense without needing much additional context. It may refer to specific technical details but can generally be understood independently.\",\n",
    "    \"score5_description\": \"The question is entirely self-contained and makes complete sense on its own. Even if technical terms or acronyms are used, a user with relevant expertise or access to documentation would understand it without needing additional context.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_prompt = \"\"\"\n",
    "Your task is to write a standalone factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "\n",
    "Context: {context}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code which evaluates QA pairs updates the original items with evaluation scores and corresponding justification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params={\n",
    "    \"temperature\": 0.0, \n",
    "    \"max_tokens\": 500,\n",
    "}\n",
    "\n",
    "judge = PrometheusEval(model=litellm_client, absolute_grade_template=prometheus_prompt)\n",
    "\n",
    "formatted_timestamp = get_timestamp()\n",
    "file_path = f'evaluated_items_{formatted_timestamp}.jsonl'\n",
    "\n",
    "for output in tqdm(outputs):\n",
    "    for metric, rubric_criteria in {\"groundedness\": groundedness_rubric_data, \"relevance\": relevance_rubric_data, \"standalone\": standalone_rubric_data}.items():\n",
    "        feedback, score = judge.single_absolute_grade(\n",
    "            instruction=instruction_prompt.format(context=' '.join(output[\"document\"][\"content\"])),       \n",
    "            response=output[\"question\"],             \n",
    "            rubric=SCORE_RUBRIC_TEMPLATE.format(**rubric_criteria),                             \n",
    "            params=params,                               \n",
    "        )\n",
    "        output.update(\n",
    "            {\n",
    "                f\"{metric}_score\": score,\n",
    "                f\"{metric}_feedback\": feedback,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    json_line = json.dumps(output)\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(json_line + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset before filtering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the name of the approach that generates both reasoning traces and task-specific actions in an interleaved manner for large language models?\\n\\n</td>\n",
       "      <td>The approach is named ReAct.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the two main sources of carbon emissions in computing?\\n</td>\n",
       "      <td>The two main sources of carbon emissions in computing are operational energy consumption and hardware manufacturing and infrastructure.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the test introduced to measure the magnitude of overall bias in neural language models?\\n</td>\n",
       "      <td>The Contextualized Embedding Association Test (CEAT).</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the name of the large language model introduced in the paper that can store, combine and reason about scientific knowledge?\\n</td>\n",
       "      <td>Galactica</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the name of the dataset collected for studying the characteristics of ChatGPT's responses and comparing them with human experts?\\n\\n</td>\n",
       "      <td>The collected dataset is called the Human ChatGPT Comparison Corpus (HC3), which contains tens of thousands of comparison responses from both human experts and ChatGPT, covering various areas such as open-domain, financial, medical, legal, and psychological questions.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>What is the name of the pre-training objective proposed in the paper that combines diverse pre-training paradigms together?\\n\\n</td>\n",
       "      <td>The pre-training objective proposed in the paper is called Mixture-of-Denoisers (MoD).</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>What is the dimensionality of the subspace that the gradient dynamically converges to in large-scale deep learning scenarios?\\n</td>\n",
       "      <td>The subspace is spanned by a few top eigenvectors of the Hessian, equal to the number of classes in the dataset.</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>What is the name of the novel method proposed to effectively leverage positional information in transformer-based language models?\\n</td>\n",
       "      <td>The novel method is named Rotary Position Embedding (RoPE).</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>What is the size range of the language models investigated in the red teaming efforts?\\n</td>\n",
       "      <td>The language models investigated in the red teaming efforts range in size from 2.7 billion parameters to 52 billion parameters.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What percentage of BIG-Bench tasks did the best model in the BIG-Bench paper outperform average reported human-rater results on via few-shot prompting?\\n\\n</td>\n",
       "      <td>The best model in the BIG-Bench paper outperformed average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                       question  \\\n",
       "0       What is the name of the approach that generates both reasoning traces and task-specific actions in an interleaved manner for large language models?\\n\\n   \n",
       "1                                                                                             What are the two main sources of carbon emissions in computing?\\n   \n",
       "2                                                 What is the name of the test introduced to measure the magnitude of overall bias in neural language models?\\n   \n",
       "3                         What is the name of the large language model introduced in the paper that can store, combine and reason about scientific knowledge?\\n   \n",
       "4                  What is the name of the dataset collected for studying the characteristics of ChatGPT's responses and comparing them with human experts?\\n\\n   \n",
       "..                                                                                                                                                          ...   \n",
       "95                              What is the name of the pre-training objective proposed in the paper that combines diverse pre-training paradigms together?\\n\\n   \n",
       "96                              What is the dimensionality of the subspace that the gradient dynamically converges to in large-scale deep learning scenarios?\\n   \n",
       "97                         What is the name of the novel method proposed to effectively leverage positional information in transformer-based language models?\\n   \n",
       "98                                                                     What is the size range of the language models investigated in the red teaming efforts?\\n   \n",
       "99  What percentage of BIG-Bench tasks did the best model in the BIG-Bench paper outperform average reported human-rater results on via few-shot prompting?\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                          answer  \\\n",
       "0                                                                                                                                                                                                                                                   The approach is named ReAct.   \n",
       "1                                                                                                                                        The two main sources of carbon emissions in computing are operational energy consumption and hardware manufacturing and infrastructure.   \n",
       "2                                                                                                                                                                                                                          The Contextualized Embedding Association Test (CEAT).   \n",
       "3                                                                                                                                                                                                                                                                      Galactica   \n",
       "4   The collected dataset is called the Human ChatGPT Comparison Corpus (HC3), which contains tens of thousands of comparison responses from both human experts and ChatGPT, covering various areas such as open-domain, financial, medical, legal, and psychological questions.   \n",
       "..                                                                                                                                                                                                                                                                           ...   \n",
       "95                                                                                                                                                                                        The pre-training objective proposed in the paper is called Mixture-of-Denoisers (MoD).   \n",
       "96                                                                                                                                                              The subspace is spanned by a few top eigenvectors of the Hessian, equal to the number of classes in the dataset.   \n",
       "97                                                                                                                                                                                                                   The novel method is named Rotary Position Embedding (RoPE).   \n",
       "98                                                                                                                                               The language models investigated in the red teaming efforts range in size from 2.7 billion parameters to 52 billion parameters.   \n",
       "99                                                                                                                                 The best model in the BIG-Bench paper outperformed average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting.   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "0                    5                5                 4  \n",
       "1                    5                3                 4  \n",
       "2                    5                4                 5  \n",
       "3                    5                5                 4  \n",
       "4                    5                5                 5  \n",
       "..                 ...              ...               ...  \n",
       "95                   5                4                 4  \n",
       "96                   5                3                 4  \n",
       "97                   5                5                 4  \n",
       "98                   5                4                 4  \n",
       "99                   5                5                 4  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "Final evaluation dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>groundedness_score</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>standalone_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the name of the test introduced to measure the magnitude of overall bias in neural language models?\\n</td>\n",
       "      <td>The Contextualized Embedding Association Test (CEAT).</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the name of the dataset collected for studying the characteristics of ChatGPT's responses and comparing them with human experts?\\n\\n</td>\n",
       "      <td>The collected dataset is called the Human ChatGPT Comparison Corpus (HC3), which contains tens of thousands of comparison responses from both human experts and ChatGPT, covering various areas such as open-domain, financial, medical, legal, and psychological questions.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the name of the dataset used in the preliminary experiments to test the neural math solver model?\\n</td>\n",
       "      <td>The dataset used in the preliminary experiments is called Math23K.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is the name of the open-source modular library introduced for optimizing language generators with reinforcement learning?\\n</td>\n",
       "      <td>The library is called RL4LMs (Reinforcement Learning for Language Models).</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the six specific risk areas associated with large-scale Language Models outlined in the paper?\\n\\n</td>\n",
       "      <td>The six specific risk areas associated with large-scale Language Models outlined in the paper are: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Automation, Access, and Environmental Harms.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>What are the key qualities necessary to build an engaging open-domain conversational agent?\\n</td>\n",
       "      <td>The key qualities necessary to build an engaging open-domain conversational agent include continual learning, providing engaging content, and being well-behaved.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What is the name of the dataset introduced to evaluate the capabilities of computational models for text understanding?\\n</td>\n",
       "      <td>LAMBADA</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>How do large language models typically generate answers?\\n</td>\n",
       "      <td>Large language models typically generate answers through a single call to the model, resulting in a lack of transparency and potentially compromising performance on multi-step problems.</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>What percentage of test cases mapped \"Muslim\" to \"terrorist\" in the GPT-3 language model?\\n</td>\n",
       "      <td>In the GPT-3 language model, \"Muslim\" was analogized to \"terrorist\" in 23% of test cases.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>How many papers from the ACL anthology were surveyed to examine the consideration of race in NLP research and development?\\n</td>\n",
       "      <td>79 papers from the ACL anthology were surveyed to examine the consideration of race in NLP research and development.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>What is the accuracy of the best method for answering yes/no questions in the BoolQ dataset?\\n</td>\n",
       "      <td>The best method, which trains BERT on MultiNLI and then re-trains it on the train set, achieves an accuracy of 80.4%.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>What is the name of the metric proposed for measuring counterfactual fairness in text classifiers?\\n</td>\n",
       "      <td>The metric proposed for measuring counterfactual fairness in text classifiers is called counterfactual token fairness (CTF).</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>What are the four machine learning domains tested in the empirical characterization of generalization error and model size growth?\\n\\n</td>\n",
       "      <td>The four machine learning domains tested are machine translation, language modeling, image processing, and speech recognition.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>What is the name of the novel training paradigm that equips large language models with multi-modal abilities through modularized learning?\\n\\n</td>\n",
       "      <td>The novel training paradigm is called mPLUG-Owl.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>What percentage of problems on the HumanEval benchmark can be solved using the hierarchical code-gen approach?\\n</td>\n",
       "      <td>39.8%</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>What is the top-1 accuracy achieved by EfficientNet-B7 on ImageNet?\\n</td>\n",
       "      <td>EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>What are the three categories used to label tweets in the study on automatic hate-speech detection on social media?\\n\\n</td>\n",
       "      <td>The three categories used to label tweets are: those containing hate speech, those containing only offensive language, and those with neither hate speech nor offensive language.</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>What is the name of the algorithm proposed for adversarial pre-training of large neural language models?\\n</td>\n",
       "      <td>The algorithm is called ALUM, which stands for Adversarial training for large neural LangUage Models.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>What are the three constituent components of the Prompt-and-Rerank method for textual style transfer?\\n</td>\n",
       "      <td>The three constituent components of the Prompt-and-Rerank method are textual similarity, target style strength, and fluency.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>What is the name of the open-source platform for Reinforcement Learning research built on top of the Android ecosystem?\\n</td>\n",
       "      <td>AndroidEnv</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>What is the name of the new family of policy gradient methods proposed for reinforcement learning?\\n</td>\n",
       "      <td>The new family of policy gradient methods is called proximal policy optimization (PPO).</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>What is the name of the largescale benchmark for commonsense reasoning about social situations introduced in the context?\\n</td>\n",
       "      <td>Social IQa</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>What is the name of the new benchmarking environment proposed for training reinforcement learning agents in web navigation tasks?\\n\\n</td>\n",
       "      <td>The new benchmarking environment is called gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites.</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>What is the name of the 20 billion parameter autoregressive language model introduced by EleutherAI?\\n</td>\n",
       "      <td>GPT-NeoX-20B</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>What is the name of the open-source framework introduced to explore and aggregate persona biases in dialogue systems?\\n\\n</td>\n",
       "      <td>UnitPersonaBias</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          question  \\\n",
       "2                                    What is the name of the test introduced to measure the magnitude of overall bias in neural language models?\\n   \n",
       "4     What is the name of the dataset collected for studying the characteristics of ChatGPT's responses and comparing them with human experts?\\n\\n   \n",
       "7                                      What is the name of the dataset used in the preliminary experiments to test the neural math solver model?\\n   \n",
       "9                 What is the name of the open-source modular library introduced for optimizing language generators with reinforcement learning?\\n   \n",
       "14                                     What are the six specific risk areas associated with large-scale Language Models outlined in the paper?\\n\\n   \n",
       "19                                                   What are the key qualities necessary to build an engaging open-domain conversational agent?\\n   \n",
       "25                       What is the name of the dataset introduced to evaluate the capabilities of computational models for text understanding?\\n   \n",
       "29                                                                                      How do large language models typically generate answers?\\n   \n",
       "30                                                     What percentage of test cases mapped \"Muslim\" to \"terrorist\" in the GPT-3 language model?\\n   \n",
       "31                    How many papers from the ACL anthology were surveyed to examine the consideration of race in NLP research and development?\\n   \n",
       "38                                                  What is the accuracy of the best method for answering yes/no questions in the BoolQ dataset?\\n   \n",
       "42                                            What is the name of the metric proposed for measuring counterfactual fairness in text classifiers?\\n   \n",
       "44          What are the four machine learning domains tested in the empirical characterization of generalization error and model size growth?\\n\\n   \n",
       "45  What is the name of the novel training paradigm that equips large language models with multi-modal abilities through modularized learning?\\n\\n   \n",
       "47                                What percentage of problems on the HumanEval benchmark can be solved using the hierarchical code-gen approach?\\n   \n",
       "48                                                                           What is the top-1 accuracy achieved by EfficientNet-B7 on ImageNet?\\n   \n",
       "55                         What are the three categories used to label tweets in the study on automatic hate-speech detection on social media?\\n\\n   \n",
       "56                                      What is the name of the algorithm proposed for adversarial pre-training of large neural language models?\\n   \n",
       "57                                         What are the three constituent components of the Prompt-and-Rerank method for textual style transfer?\\n   \n",
       "64                       What is the name of the open-source platform for Reinforcement Learning research built on top of the Android ecosystem?\\n   \n",
       "66                                            What is the name of the new family of policy gradient methods proposed for reinforcement learning?\\n   \n",
       "72                     What is the name of the largescale benchmark for commonsense reasoning about social situations introduced in the context?\\n   \n",
       "79           What is the name of the new benchmarking environment proposed for training reinforcement learning agents in web navigation tasks?\\n\\n   \n",
       "83                                          What is the name of the 20 billion parameter autoregressive language model introduced by EleutherAI?\\n   \n",
       "92                       What is the name of the open-source framework introduced to explore and aggregate persona biases in dialogue systems?\\n\\n   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                             answer  \\\n",
       "2                                                                                                                                                                                                                                                             The Contextualized Embedding Association Test (CEAT).   \n",
       "4                                      The collected dataset is called the Human ChatGPT Comparison Corpus (HC3), which contains tens of thousands of comparison responses from both human experts and ChatGPT, covering various areas such as open-domain, financial, medical, legal, and psychological questions.   \n",
       "7                                                                                                                                                                                                                                                The dataset used in the preliminary experiments is called Math23K.   \n",
       "9                                                                                                                                                                                                                                        The library is called RL4LMs (Reinforcement Learning for Language Models).   \n",
       "14  The six specific risk areas associated with large-scale Language Models outlined in the paper are: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Automation, Access, and Environmental Harms.   \n",
       "19                                                                                                                                                The key qualities necessary to build an engaging open-domain conversational agent include continual learning, providing engaging content, and being well-behaved.   \n",
       "25                                                                                                                                                                                                                                                                                                          LAMBADA   \n",
       "29                                                                                                                        Large language models typically generate answers through a single call to the model, resulting in a lack of transparency and potentially compromising performance on multi-step problems.   \n",
       "30                                                                                                                                                                                                                        In the GPT-3 language model, \"Muslim\" was analogized to \"terrorist\" in 23% of test cases.   \n",
       "31                                                                                                                                                                                             79 papers from the ACL anthology were surveyed to examine the consideration of race in NLP research and development.   \n",
       "38                                                                                                                                                                                            The best method, which trains BERT on MultiNLI and then re-trains it on the train set, achieves an accuracy of 80.4%.   \n",
       "42                                                                                                                                                                                     The metric proposed for measuring counterfactual fairness in text classifiers is called counterfactual token fairness (CTF).   \n",
       "44                                                                                                                                                                                   The four machine learning domains tested are machine translation, language modeling, image processing, and speech recognition.   \n",
       "45                                                                                                                                                                                                                                                                 The novel training paradigm is called mPLUG-Owl.   \n",
       "47                                                                                                                                                                                                                                                                                                            39.8%   \n",
       "48                                                                                                                                                                                                                                      EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet.   \n",
       "55                                                                                                                                The three categories used to label tweets are: those containing hate speech, those containing only offensive language, and those with neither hate speech nor offensive language.   \n",
       "56                                                                                                                                                                                                            The algorithm is called ALUM, which stands for Adversarial training for large neural LangUage Models.   \n",
       "57                                                                                                                                                                                     The three constituent components of the Prompt-and-Rerank method are textual similarity, target style strength, and fluency.   \n",
       "64                                                                                                                                                                                                                                                                                                       AndroidEnv   \n",
       "66                                                                                                                                                                                                                          The new family of policy gradient methods is called proximal policy optimization (PPO).   \n",
       "72                                                                                                                                                                                                                                                                                                       Social IQa   \n",
       "79                                                                                                                                            The new benchmarking environment is called gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites.   \n",
       "83                                                                                                                                                                                                                                                                                                     GPT-NeoX-20B   \n",
       "92                                                                                                                                                                                                                                                                                                  UnitPersonaBias   \n",
       "\n",
       "    groundedness_score  relevance_score  standalone_score  \n",
       "2                    5                4                 5  \n",
       "4                    5                5                 5  \n",
       "7                    5                4                 5  \n",
       "9                    5                5                 5  \n",
       "14                   5                5                 5  \n",
       "19                   5                4                 5  \n",
       "25                   5                5                 5  \n",
       "29                   4                4                 5  \n",
       "30                   5                5                 5  \n",
       "31                   5                5                 5  \n",
       "38                   5                4                 5  \n",
       "42                   5                5                 5  \n",
       "44                   5                5                 5  \n",
       "45                   5                5                 5  \n",
       "47                   5                5                 5  \n",
       "48                   5                5                 5  \n",
       "55                   5                4                 5  \n",
       "56                   5                5                 5  \n",
       "57                   5                5                 5  \n",
       "64                   5                5                 5  \n",
       "66                   5                4                 5  \n",
       "72                   5                5                 5  \n",
       "79                   5                5                 5  \n",
       "83                   5                5                 5  \n",
       "92                   5                5                 5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "\n",
    "print(\"Evaluation dataset before filtering:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "generated_questions = generated_questions.loc[\n",
    "    (generated_questions[\"groundedness_score\"] >= 4)\n",
    "    & (generated_questions[\"relevance_score\"] >= 4)\n",
    "    & (generated_questions[\"standalone_score\"] >= 5)\n",
    "]\n",
    "print(\"============================================\")\n",
    "print(\"Final evaluation dataset:\")\n",
    "display(\n",
    "    generated_questions[\n",
    "        [\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "            \"groundedness_score\",\n",
    "            \"relevance_score\",\n",
    "            \"standalone_score\",\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "eval_dataset = datasets.Dataset.from_pandas(generated_questions, split=\"train\", preserve_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we have only 1/4 of the generated QA pairs which scored 5/5 on every metric. Proceed to save the filtered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 25/25 [00:00<00:00, 3591.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dataset.save_to_disk(\"NLP_eval_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-POwRF2H0-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
