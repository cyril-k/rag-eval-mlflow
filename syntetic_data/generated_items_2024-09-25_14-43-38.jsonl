{"document": {"content": "While large language models (LLMs) have demonstrated impressive capabilities\nacross tasks in language understanding and interactive decision making, their\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\naction plan generation) have primarily been studied as separate topics. In this\npaper, we explore the use of LLMs to generate both reasoning traces and\ntask-specific actions in an interleaved manner, allowing for greater synergy\nbetween the two: reasoning traces help the model induce, track, and update\naction plans as well as handle exceptions, while actions allow it to interface\nwith external sources, such as knowledge bases or environments, to gather\nadditional information. We apply our approach, named ReAct, to a diverse set of\nlanguage and decision making tasks and demonstrate its effectiveness over\nstate-of-the-art baselines, as well as improved human interpretability and\ntrustworthiness over methods without reasoning or acting components.\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\nReAct overcomes issues of hallucination and error propagation prevalent in\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\ngenerates human-like task-solving trajectories that are more interpretable than\nbaselines without reasoning traces. On two interactive decision making\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\nreinforcement learning methods by an absolute success rate of 34% and 10%\nrespectively, while being prompted with only one or two in-context examples.\nProject site with code: https://react-lm.github.io", "collection_id": "8492206a4700a9315b3e01f28300f185bdd9e42e7bfd932186437f612ef00def"}, "question": "What is the name of the approach that generates both reasoning traces and task-specific actions in an interleaved manner for large language models?\n\n", "answer": "The approach is named ReAct."}
{"document": {"content": "Given recent algorithm, software, and hardware innovation, computing has\nenabled a plethora of new applications. As computing becomes increasingly\nubiquitous, however, so does its environmental impact. This paper brings the\nissue to the attention of computer-systems researchers. Our analysis, built on\nindustry-reported characterization, quantifies the environmental effects of\ncomputing in terms of carbon emissions. Broadly, carbon emissions have two\nsources: operational energy consumption, and hardware manufacturing and\ninfrastructure. Although carbon emissions from the former are decreasing thanks\nto algorithmic, software, and hardware innovations that boost performance and\npower efficiency, the overall carbon footprint of computer systems continues to\ngrow. This work quantifies the carbon output of computer systems to show that\nmost emissions related to modern mobile and data-center equipment come from\nhardware manufacturing and infrastructure. We therefore outline future\ndirections for minimizing the environmental impact of computing systems.", "collection_id": "8d09d355e882471b36c68c9504b240264abc04fe57475a66eaecd2cf343ef86f"}, "question": "What are the two main sources of carbon emissions in computing?\n", "answer": "The two main sources of carbon emissions in computing are operational energy consumption and hardware manufacturing and infrastructure."}
{"document": {"content": "With the starting point that implicit human biases are reflected in the\nstatistical regularities of language, it is possible to measure biases in\nEnglish static word embeddings. State-of-the-art neural language models\ngenerate dynamic word embeddings dependent on the context in which the word\nappears. Current methods measure pre-defined social and intersectional biases\nthat appear in particular contexts defined by sentence templates. Dispensing\nwith templates, we introduce the Contextualized Embedding Association Test\n(CEAT), that can summarize the magnitude of overall bias in neural language\nmodels by incorporating a random-effects model. Experiments on social and\nintersectional biases show that CEAT finds evidence of all tested biases and\nprovides comprehensive information on the variance of effect magnitudes of the\nsame bias in different contexts. All the models trained on English corpora that\nwe study contain biased representations.\n  Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and\nEmergent Intersectional Bias Detection (EIBD), to automatically identify the\nintersectional biases and emergent intersectional biases from static word\nembeddings in addition to measuring them in contextualized word embeddings. We\npresent the first algorithmic bias detection findings on how intersectional\ngroup members are strongly associated with unique emergent biases that do not\noverlap with the biases of their constituent minority identities. IBD and EIBD\nachieve high accuracy when detecting the intersectional and emergent biases of\nAfrican American females and Mexican American females. Our results indicate\nthat biases at the intersection of race and gender associated with members of\nmultiple minority groups, such as African American females and Mexican American\nfemales, have the highest magnitude across all neural language models.", "collection_id": "d6eb0b3edbcbd10ae6ec43c881bf1fe35c10bda62356ef4b1c6affdd5c925015"}, "question": "What is the name of the test introduced to measure the magnitude of overall bias in neural language models?\n", "answer": "The Contextualized Embedding Association Test (CEAT)."}
{"document": {"content": "Information overload is a major obstacle to scientific progress. The\nexplosive growth in scientific literature and data has made it ever harder to\ndiscover useful insights in a large mass of information. Today scientific\nknowledge is accessed through search engines, but they are unable to organize\nscientific knowledge alone. In this paper we introduce Galactica: a large\nlanguage model that can store, combine and reason about scientific knowledge.\nWe train on a large scientific corpus of papers, reference material, knowledge\nbases and many other sources. We outperform existing models on a range of\nscientific tasks. On technical knowledge probes such as LaTeX equations,\nGalactica outperforms the latest GPT-3 by 68.2% versus 49.0%. Galactica also\nperforms well on reasoning, outperforming Chinchilla on mathematical MMLU by\n41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%. It\nalso sets a new state-of-the-art on downstream tasks such as PubMedQA and\nMedMCQA dev of 77.6% and 52.9%. And despite not being trained on a general\ncorpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these\nresults demonstrate the potential for language models as a new interface for\nscience. We open source the model for the benefit of the scientific community.", "collection_id": "5317349bd3688c890ccf102dc36d8da189246b2f358d31b3ee5e875183a80206"}, "question": "What is the name of the large language model introduced in the paper that can store, combine and reason about scientific knowledge?\n", "answer": "Galactica"}
{"document": {"content": "The introduction of ChatGPT has garnered widespread attention in both\nacademic and industrial communities. ChatGPT is able to respond effectively to\na wide range of human questions, providing fluent and comprehensive answers\nthat significantly surpass previous public chatbots in terms of security and\nusefulness. On one hand, people are curious about how ChatGPT is able to\nachieve such strength and how far it is from human experts. On the other hand,\npeople are starting to worry about the potential negative impacts that large\nlanguage models (LLMs) like ChatGPT could have on society, such as fake news,\nplagiarism, and social security issues. In this work, we collected tens of\nthousands of comparison responses from both human experts and ChatGPT, with\nquestions ranging from open-domain, financial, medical, legal, and\npsychological areas. We call the collected dataset the Human ChatGPT Comparison\nCorpus (HC3). Based on the HC3 dataset, we study the characteristics of\nChatGPT's responses, the differences and gaps from human experts, and future\ndirections for LLMs. We conducted comprehensive human evaluations and\nlinguistic analyses of ChatGPT-generated content compared with that of humans,\nwhere many interesting results are revealed. After that, we conduct extensive\nexperiments on how to effectively detect whether a certain text is generated by\nChatGPT or humans. We build three different detection systems, explore several\nkey factors that influence their effectiveness, and evaluate them in different\nscenarios. The dataset, code, and models are all publicly available at\nhttps://github.com/Hello-SimpleAI/chatgpt-comparison-detection.", "collection_id": "14ca7dc42f6520b4c914587fd8c0bee64f90574d35e3818c477510e5a0f1ef83"}, "question": "What is the name of the dataset collected for studying the characteristics of ChatGPT's responses and comparing them with human experts?\n\n", "answer": "The collected dataset is called the Human ChatGPT Comparison Corpus (HC3), which contains tens of thousands of comparison responses from both human experts and ChatGPT, covering various areas such as open-domain, financial, medical, legal, and psychological questions."}
{"document": {"content": "While large-scale language models (LMs) are able to imitate the distribution\nof natural language well enough to generate realistic text, it is difficult to\ncontrol which regions of the distribution they generate. This is especially\nproblematic because datasets used for training large LMs usually contain\nsignificant toxicity, hate, bias, and negativity. We propose GeDi as an\nefficient method for using smaller LMs as generative discriminators to guide\ngeneration from large LMs to make them safer and more controllable. GeDi guides\ngeneration at each step by computing classification probabilities for all\npossible next tokens via Bayes rule by normalizing over two class-conditional\ndistributions; one conditioned on the desired attribute, or control code, and\nanother conditioned on the undesired attribute, or anti control code. We find\nthat GeDi gives stronger controllability than the state of the art method while\nalso achieving generation speeds more than 30 times faster. Additionally,\ntraining GeDi on only four topics allows us to controllably generate new topics\nzero-shot from just a keyword, unlocking a new capability that previous\ncontrollable generation methods do not have. Lastly, we show that GeDi can make\nGPT-2 (1.5B parameters) significantly less toxic without sacrificing linguistic\nquality, making it by far the most practical existing method for detoxifying\nlarge language models while maintaining a fast generation speed.", "collection_id": "205b1f5b415327bf504c657e108b195b78460574eb207ff52a5da3dc9ff98691"}, "question": "How much faster is GeDi's generation speed compared to the state of the art method?\n", "answer": "GeDi achieves generation speeds more than 30 times faster than the state of the art method."}
{"document": {"content": "In this paper, we propose a simple yet effective way to generate pun\nsentences that does not require any training on existing puns. Our approach is\ninspired by humor theories that ambiguity comes from the context rather than\nthe pun word itself. Given a pair of definitions of a pun word, our model first\nproduces a list of related concepts through a reverse dictionary. We then\nutilize one-shot GPT3 to generate context words and then generate puns\nincorporating context words from both concepts. Human evaluation shows that our\nmethod successfully generates pun 52\\% of the time, outperforming well-crafted\nbaselines and the state-of-the-art models by a large margin.", "collection_id": "07f6c9f82751f0c3328c6cb7c82d1c5589096f07ab23984815f6a1570b7ac844"}, "question": "What is the success rate of the proposed method in generating puns?\n", "answer": "The proposed method successfully generates puns 52% of the time, outperforming well-crafted baselines and state-of-the-art models by a large margin."}
{"document": {"content": "Solving math word problems is a challenging task that requires accurate\nnatural language understanding to bridge natural language texts and math\nexpressions. Motivated by the intuition about how human generates the equations\ngiven the problem texts, this paper presents a neural approach to automatically\nsolve math word problems by operating symbols according to their semantic\nmeanings in texts. This paper views the process of generating equation as a\nbridge between the semantic world and the symbolic world, where the proposed\nneural math solver is based on an encoder-decoder framework. In the proposed\nmodel, the encoder is designed to understand the semantics of problems, and the\ndecoder focuses on tracking semantic meanings of the generated symbols and then\ndeciding which symbol to generate next. The preliminary experiments are\nconducted in a dataset Math23K, and our model significantly outperforms both\nthe state-of-the-art single model and the best non-retrieval-based model over\nabout 10% accuracy, demonstrating the effectiveness of bridging the symbolic\nand semantic worlds from math word problems.", "collection_id": "89b8b7c839330f8b7f10b786e830096e922e18e670f4bb5efb16508c9cfbdf5b"}, "question": "What is the name of the dataset used in the preliminary experiments to test the neural math solver model?\n", "answer": "The dataset used in the preliminary experiments is called Math23K."}
{"document": {"content": "Recent progress in hardware and methodology for training neural networks has\nushered in a new generation of large networks trained on abundant data. These\nmodels have obtained notable gains in accuracy across many NLP tasks. However,\nthese accuracy improvements depend on the availability of exceptionally large\ncomputational resources that necessitate similarly substantial energy\nconsumption. As a result these models are costly to train and develop, both\nfinancially, due to the cost of hardware and electricity or cloud compute time,\nand environmentally, due to the carbon footprint required to fuel modern tensor\nprocessing hardware. In this paper we bring this issue to the attention of NLP\nresearchers by quantifying the approximate financial and environmental costs of\ntraining a variety of recently successful neural network models for NLP. Based\non these findings, we propose actionable recommendations to reduce costs and\nimprove equity in NLP research and practice.", "collection_id": "534ecede101b6cb78b13fcf0af774a47f87d60aa004807f6ba574afa27ab5936"}, "question": "What are the drawbacks of training large neural networks for NLP tasks?\n", "answer": "The drawbacks of training large neural networks for NLP tasks are the high financial costs due to hardware and electricity or cloud compute time, and the substantial environmental costs due to the carbon footprint required to fuel modern tensor processing hardware."}
{"document": {"content": "We tackle the problem of aligning pre-trained large language models (LMs)\nwith human preferences. If we view text generation as a sequential\ndecision-making problem, reinforcement learning (RL) appears to be a natural\nconceptual framework. However, using RL for LM-based generation faces empirical\nchallenges, including training instability due to the combinatorial action\nspace, as well as a lack of open-source libraries and benchmarks customized for\nLM alignment. Thus, a question rises in the research community: is RL a\npractical paradigm for NLP?\n  To help answer this, we first introduce an open-source modular library,\nRL4LMs (Reinforcement Learning for Language Models), for optimizing language\ngenerators with RL. The library consists of on-policy RL algorithms that can be\nused to train any encoder or encoder-decoder LM in the HuggingFace library\n(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE\n(General Reinforced-language Understanding Evaluation) benchmark, a set of 6\nlanguage generation tasks which are supervised not by target strings, but by\nreward functions which capture automated measures of human preference.GRUE is\nthe first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,\nwe introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language\nPolicy Optimization)} that learns to effectively reduce the combinatorial\naction space in language generation. We show 1) that RL techniques are\ngenerally better than supervised methods at aligning LMs to human preferences;\nand 2) that NLPO exhibits greater stability and performance than previous\npolicy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both\nautomatic and human evaluations.", "collection_id": "31ecb2a6780cc7326f6984eabefdb2364352a8d3217054b639a033703aa21b0b"}, "question": "What is the name of the open-source modular library introduced for optimizing language generators with reinforcement learning?\n", "answer": "The library is called RL4LMs (Reinforcement Learning for Language Models)."}
{"document": {"content": "Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.", "collection_id": "26327c6b9c4122855f0cf3c1a8e22f949c32c278ab2f3b43ddb253e75f047c20"}, "question": "What is the name of the model introduced in the paper that can teach itself to use external tools via simple APIs?\n", "answer": "The model is called Toolformer, which is trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction."}
{"document": {"content": "As Transfer Learning from large-scale pre-trained models becomes more\nprevalent in Natural Language Processing (NLP), operating these large models in\non-the-edge and/or under constrained computational training or inference\nbudgets remains challenging. In this work, we propose a method to pre-train a\nsmaller general-purpose language representation model, called DistilBERT, which\ncan then be fine-tuned with good performances on a wide range of tasks like its\nlarger counterparts. While most prior work investigated the use of distillation\nfor building task-specific models, we leverage knowledge distillation during\nthe pre-training phase and show that it is possible to reduce the size of a\nBERT model by 40%, while retaining 97% of its language understanding\ncapabilities and being 60% faster. To leverage the inductive biases learned by\nlarger models during pre-training, we introduce a triple loss combining\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\nfor on-device computations in a proof-of-concept experiment and a comparative\non-device study.", "collection_id": "536ba815697d1dfadfe1bed9ef3417784cbf307e152435dd5f3896e412240f89"}, "question": "How much smaller is the DistilBERT model compared to its larger BERT counterpart?\n", "answer": "The DistilBERT model is 40% smaller than its larger BERT counterpart, while retaining 97% of its language understanding capabilities and being 60% faster."}
{"document": {"content": "Machine learning and NLP require the construction of datasets to train and\nfine-tune models. In this context, previous work has demonstrated the\nsensitivity of these data sets. For instance, potential societal biases in this\ndata are likely to be encoded and to be amplified in the models we deploy. In\nthis work, we draw from developments in the field of history and take a novel\nperspective on these problems: considering datasets and models through the lens\nof historical fiction surfaces their political nature, and affords\nre-configuring how we view the past, such that marginalized discourses are\nsurfaced. Building on such insights, we argue that contemporary methods for\nmachine learning are prejudiced towards dominant and hegemonic histories.\nEmploying the example of neopronouns, we show that by surfacing marginalized\nhistories within contemporary conditions, we can create models that better\nrepresent the lived realities of traditionally marginalized and excluded\ncommunities.", "collection_id": "e26732b1647fdce64c1c44b10e3ac56884aa211cb2a2fa4a37ddf14fc23ea9d5"}, "question": "What is the potential issue with the datasets used to train machine learning and NLP models?\n", "answer": "The potential issue is that they may encode and amplify societal biases, particularly those that favor dominant and hegemonic histories, which can lead to the marginalization of already excluded communities."}
{"document": {"content": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.", "collection_id": "f80c7c3aa15ff4b4c74e44c23979b2527e2a345120e03ebb13cedf18520f3c16"}, "question": "What is the name of the model that was found to be significantly undertrained in the replication study of BERT pretraining?\n", "answer": "BERT"}
{"document": {"content": "This paper aims to help structure the risk landscape associated with\nlarge-scale Language Models (LMs). In order to foster advances in responsible\ninnovation, an in-depth understanding of the potential risks posed by these\nmodels is needed. A wide range of established and anticipated risks are\nanalysed in detail, drawing on multidisciplinary expertise and literature from\ncomputer science, linguistics, and social sciences.\n  We outline six specific risk areas: I. Discrimination, Exclusion and\nToxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious\nUses, V. Human-Computer Interaction Harms, VI. Automation, Access, and\nEnvironmental Harms. The first area concerns the perpetuation of stereotypes,\nunfair discrimination, exclusionary norms, toxic language, and lower\nperformance by social group for LMs. The second focuses on risks from private\ndata leaks or LMs correctly inferring sensitive information. The third\naddresses risks arising from poor, false or misleading information including in\nsensitive domains, and knock-on risks such as the erosion of trust in shared\ninformation. The fourth considers risks from actors who try to use LMs to cause\nharm. The fifth focuses on risks specific to LLMs used to underpin\nconversational agents that interact with human users, including unsafe use,\nmanipulation or deception. The sixth discusses the risk of environmental harm,\njob automation, and other challenges that may have a disparate effect on\ndifferent social groups or communities.\n  In total, we review 21 risks in-depth. We discuss the points of origin of\ndifferent risks and point to potential mitigation approaches. Lastly, we\ndiscuss organisational responsibilities in implementing mitigations, and the\nrole of collaboration and participation. We highlight directions for further\nresearch, particularly on expanding the toolkit for assessing and evaluating\nthe outlined risks in LMs.", "collection_id": "04b47d3650202378915dcbd2c02611c433de4906c95f8c2f887a6e4d92eea080"}, "question": "What are the six specific risk areas associated with large-scale Language Models outlined in the paper?\n\n", "answer": "The six specific risk areas associated with large-scale Language Models outlined in the paper are: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Automation, Access, and Environmental Harms."}
{"document": {"content": "Sophisticated language models such as OpenAI's GPT-3 can generate hateful\ntext that targets marginalized groups. Given this capacity, we are interested\nin whether large language models can be used to identify hate speech and\nclassify text as sexist or racist. We use GPT-3 to identify sexist and racist\ntext passages with zero-, one-, and few-shot learning. We find that with zero-\nand one-shot learning, GPT-3 can identify sexist or racist text with an average\naccuracy between 55 per cent and 67 per cent, depending on the category of text\nand type of learning. With few-shot learning, the model's accuracy can be as\nhigh as 85 per cent. Large language models have a role to play in hate speech\ndetection, and with further development they could eventually be used to\ncounter hate speech.", "collection_id": "d39f018b3da20ca0c16536a150dca89ff41205396151b45662493754966ba31a"}, "question": "What is the highest accuracy rate achieved by GPT-3 in identifying sexist or racist text with few-shot learning?\n", "answer": "The highest accuracy rate achieved by GPT-3 in identifying sexist or racist text with few-shot learning is as high as 85 per cent."}
{"document": {"content": "As AI systems become increasingly powerful and pervasive, there are growing\nconcerns about machines' morality or a lack thereof. Yet, teaching morality to\nmachines is a formidable task, as morality remains among the most intensely\ndebated questions in humanity, let alone for AI. Existing AI systems deployed\nto millions of users, however, are already making decisions loaded with moral\nimplications, which poses a seemingly impossible challenge: teaching machines\nmoral sense, while humanity continues to grapple with it.\n  To explore this challenge, we introduce Delphi, an experimental framework\nbased on deep neural networks trained directly to reason about descriptive\nethical judgments, e.g., \"helping a friend\" is generally good, while \"helping a\nfriend spread fake news\" is not. Empirical results shed novel insights on the\npromises and limits of machine ethics; Delphi demonstrates strong\ngeneralization capabilities in the face of novel ethical situations, while\noff-the-shelf neural network models exhibit markedly poor judgment including\nunjust biases, confirming the need for explicitly teaching machines moral\nsense.\n  Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and\ninconsistencies. Despite that, we demonstrate positive use cases of imperfect\nDelphi, including using it as a component model within other imperfect AI\nsystems. Importantly, we interpret the operationalization of Delphi in light of\nprominent ethical theories, which leads us to important future research\nquestions.", "collection_id": "d8bfca50f0b34118e97e784fa6a27c130e79f0f399d0c716b8b2fd6bb2d3aefb"}, "question": "What is the name of the experimental framework based on deep neural networks designed to reason about descriptive ethical judgments?\n\n", "answer": "Delphi"}
{"document": {"content": "Social media data has become crucial to the advancement of scientific\nunderstanding. However, even though it has become ubiquitous, just collecting\nlarge-scale social media data involves a high degree of engineering skill set\nand computational resources. In fact, research is often times gated by data\nengineering problems that must be overcome before analysis can proceed. This\nhas resulted recognition of datasets as meaningful research contributions in\nand of themselves. Reddit, the so called \"front page of the Internet,\" in\nparticular has been the subject of numerous scientific studies. Although Reddit\nis relatively open to data acquisition compared to social media platforms like\nFacebook and Twitter, the technical barriers to acquisition still remain. Thus,\nReddit's millions of subreddits, hundreds of millions of users, and hundreds of\nbillions of comments are at the same time relatively accessible, but time\nconsuming to collect and analyze systematically. In this paper, we present the\nPushshift Reddit dataset. Pushshift is a social media data collection,\nanalysis, and archiving platform that since 2015 has collected Reddit data and\nmade it available to researchers. Pushshift's Reddit dataset is updated in\nreal-time, and includes historical data back to Reddit's inception. In addition\nto monthly dumps, Pushshift provides computational tools to aid in searching,\naggregating, and performing exploratory analysis on the entirety of the\ndataset. The Pushshift Reddit dataset makes it possible for social media\nresearchers to reduce time spent in the data collection, cleaning, and storage\nphases of their projects.", "collection_id": "92f7bf1adea18948ddf6cb9063e40dc1b776f5d10fddd8251cd3fd8e6798b4b3"}, "question": "When did Pushshift start collecting Reddit data?\n", "answer": "Pushshift started collecting Reddit data in 2015."}
{"document": {"content": "This paper presents the first consumer-scale next-word prediction (NWP) model\ntrained with Federated Learning (FL) while leveraging the Differentially\nPrivate Federated Averaging (DP-FedAvg) technique. There has been prior work on\nbuilding practical FL infrastructure, including work demonstrating the\nfeasibility of training language models on mobile devices using such\ninfrastructure. It has also been shown (in simulations on a public corpus) that\nit is possible to train NWP models with user-level differential privacy using\nthe DP-FedAvg algorithm. Nevertheless, training production-quality NWP models\nwith DP-FedAvg in a real-world production environment on a heterogeneous fleet\nof mobile phones requires addressing numerous challenges. For instance, the\ncoordinating central server has to keep track of the devices available at the\nstart of each round and sample devices uniformly at random from them, while\nensuring \\emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL\nwork of which we are aware, for the first time we demonstrate the deployment of\na differentially private mechanism for the training of a production neural\nnetwork in FL, as well as the instrumentation of the production training\ninfrastructure to perform an end-to-end empirical measurement of unintended\nmemorization.", "collection_id": "2cb926b03f70b1076b20f7f961e94a7d32ec0568883c4fe79dfb93ce6d3492bf"}, "question": "What technique is used to train the next-word prediction model in the paper?\n", "answer": "The Differentially Private Federated Averaging (DP-FedAvg) technique is used to train the next-word prediction model in the paper."}
{"document": {"content": "We present our view of what is necessary to build an engaging open-domain\nconversational agent: covering the qualities of such an agent, the pieces of\nthe puzzle that have been built so far, and the gaping holes we have not filled\nyet. We present a biased view, focusing on work done by our own group, while\nciting related work in each area. In particular, we discuss in detail the\nproperties of continual learning, providing engaging content, and being\nwell-behaved -- and how to measure success in providing them. We end with a\ndiscussion of our experience and learnings, and our recommendations to the\ncommunity.", "collection_id": "8e19eb7da594390db66ebe531954dc11158f0c3d2bc21520d401978b73a9429b"}, "question": "What are the key qualities necessary to build an engaging open-domain conversational agent?\n", "answer": "The key qualities necessary to build an engaging open-domain conversational agent include continual learning, providing engaging content, and being well-behaved."}
{"document": {"content": "In this work we create agents that can perform well beyond a single,\nindividual task, that exhibit much wider generalisation of behaviour to a\nmassive, rich space of challenges. We define a universe of tasks within an\nenvironment domain and demonstrate the ability to train agents that are\ngenerally capable across this vast space and beyond. The environment is\nnatively multi-agent, spanning the continuum of competitive, cooperative, and\nindependent games, which are situated within procedurally generated physical 3D\nworlds. The resulting space is exceptionally diverse in terms of the challenges\nposed to agents, and as such, even measuring the learning progress of an agent\nis an open research problem. We propose an iterative notion of improvement\nbetween successive generations of agents, rather than seeking to maximise a\nsingular objective, allowing us to quantify progress despite tasks being\nincomparable in terms of achievable rewards. We show that through constructing\nan open-ended learning process, which dynamically changes the training task\ndistributions and training objectives such that the agent never stops learning,\nwe achieve consistent learning of new behaviours. The resulting agent is able\nto score reward in every one of our humanly solvable evaluation levels, with\nbehaviour generalising to many held-out points in the universe of tasks.\nExamples of this zero-shot generalisation include good performance on Hide and\nSeek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks\nwe characterise the behaviour of our agent, and find interesting emergent\nheuristic behaviours such as trial-and-error experimentation, simple tool use,\noption switching, and cooperation. Finally, we demonstrate that the general\ncapabilities of this agent could unlock larger scale transfer of behaviour\nthrough cheap finetuning.", "collection_id": "e94d38f533f026658bc384240d546591c5613cc678b31afbf978658825f27b19"}, "question": "What types of games are situated within the procedurally generated physical 3D worlds in the environment domain?\n\n", "answer": "The environment domain includes a continuum of competitive, cooperative, and independent games, such as Hide and Seek, Capture the Flag, and Tag, which are situated within procedurally generated physical 3D worlds."}
{"document": {"content": "Few-shot prompting is a surprisingly powerful way to use Large Language\nModels (LLMs) to solve various tasks. However, this approach struggles as the\ntask complexity increases or when the individual reasoning steps of the task\nthemselves are hard to learn, especially when embedded in more complex tasks.\nTo address this, we propose Decomposed Prompting, a new approach to solve\ncomplex tasks by decomposing them (via prompting) into simpler sub-tasks that\ncan be delegated to a library of prompting-based LLMs dedicated to these\nsub-tasks. This modular structure allows each prompt to be optimized for its\nspecific sub-task, further decomposed if necessary, and even easily replaced\nwith more effective prompts, trained models, or symbolic functions if desired.\nWe show that the flexibility and modularity of Decomposed Prompting allows it\nto outperform prior work on few-shot prompting using GPT3. On symbolic\nreasoning tasks, we can further decompose sub-tasks that are hard for LLMs into\neven simpler solvable sub-tasks. When the complexity comes from the input\nlength, we can recursively decompose the task into the same task but with\nsmaller inputs. We also evaluate our approach on textual multi-step reasoning\ntasks: on long-context multi-hop QA task, we can more effectively teach the\nsub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA,\nwe can incorporate a symbolic information retrieval within our decomposition\nframework, leading to improved performance on both tasks. Datasets, Code and\nPrompts available at https://github.com/allenai/DecomP.", "collection_id": "e0e13f3b213c9a4ad72226a003c2ae531d2b3fe9e328e9baf3e56951443ba845"}, "question": "What is Decomposed Prompting and how does it improve the performance of Large Language Models?\n", "answer": "Decomposed Prompting is a new approach that solves complex tasks by decomposing them into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. This approach improves the performance of LLMs by allowing for greater flexibility and modularity, enabling it to outperform prior work on few-shot prompting using GPT3."}
{"document": {"content": "Advances in language modeling architectures and the availability of large\ntext corpora have driven progress in automatic text generation. While this\nresults in models capable of generating coherent texts, it also prompts models\nto internalize social biases present in the training corpus. This paper aims to\nquantify and reduce a particular type of bias exhibited by language models:\nbias in the sentiment of generated text. Given a conditioning context (e.g., a\nwriting prompt) and a language model, we analyze if (and how) the sentiment of\nthe generated text is affected by changes in values of sensitive attributes\n(e.g., country names, occupations, genders) in the conditioning context using a\nform of counterfactual evaluation. We quantify sentiment bias by adopting\nindividual and group fairness metrics from the fair machine learning\nliterature, and demonstrate that large-scale models trained on two different\ncorpora (news articles, and Wikipedia) exhibit considerable levels of bias. We\nthen propose embedding and sentiment prediction-derived regularization on the\nlanguage model's latent representations. The regularizations improve fairness\nmetrics while retaining comparable levels of perplexity and semantic\nsimilarity.", "collection_id": "3581ca4790aeae49ecacebd249c1be6c7f543075855dd7803bd43ed8214e7d4d"}, "question": "What type of bias in language models does the paper aim to quantify and reduce?\n\n", "answer": "The paper aims to quantify and reduce bias in the sentiment of generated text, specifically how the sentiment is affected by changes in values of sensitive attributes in the conditioning context."}
{"document": {"content": "Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.", "collection_id": "467089940b89b390f118b263e50abeffb793892ebeeea3e3b39e2cce58762953"}, "question": "What type of models are used for the decoder in the proposed two-stage model for image generation?\n", "answer": "Diffusion models are used for the decoder in the proposed two-stage model for image generation."}
{"document": {"content": "Recently, neural network based dialogue systems have become ubiquitous in our\nincreasingly digitalized society. However, due to their inherent opaqueness,\nsome recently raised concerns about using neural models are starting to be\ntaken seriously. In fact, intentional or unintentional behaviors could lead to\na dialogue system to generate inappropriate responses. Thus, in this paper, we\ninvestigate whether we can learn to craft input sentences that result in a\nblack-box neural dialogue model being manipulated into having its outputs\ncontain target words or match target sentences. We propose a reinforcement\nlearning based model that can generate such desired inputs automatically.\nExtensive experiments on a popular well-trained state-of-the-art neural\ndialogue model show that our method can successfully seek out desired inputs\nthat lead to the target outputs in a considerable portion of cases.\nConsequently, our work reveals the potential of neural dialogue models to be\nmanipulated, which inspires and opens the door towards developing strategies to\ndefend them.", "collection_id": "ad494d9f5a8b682e1ff5777b5cfd0e2956b956c3261b693e9dd4957a69c2336f"}, "question": "What is the main concern with using neural network based dialogue systems?\n", "answer": "The main concern is that these systems can generate inappropriate responses due to intentional or unintentional behaviors, highlighting the need to develop strategies to defend them against manipulation."}
{"document": {"content": "We introduce LAMBADA, a dataset to evaluate the capabilities of computational\nmodels for text understanding by means of a word prediction task. LAMBADA is a\ncollection of narrative passages sharing the characteristic that human subjects\nare able to guess their last word if they are exposed to the whole passage, but\nnot if they only see the last sentence preceding the target word. To succeed on\nLAMBADA, computational models cannot simply rely on local context, but must be\nable to keep track of information in the broader discourse. We show that\nLAMBADA exemplifies a wide range of linguistic phenomena, and that none of\nseveral state-of-the-art language models reaches accuracy above 1% on this\nnovel benchmark. We thus propose LAMBADA as a challenging test set, meant to\nencourage the development of new models capable of genuine understanding of\nbroad context in natural language text.", "collection_id": "e77dbc7d6319f136af2af949929e6e013cd33d7ad96fa8983c4fd79efcf3d278"}, "question": "What is the name of the dataset introduced to evaluate the capabilities of computational models for text understanding?\n", "answer": "LAMBADA"}
{"document": {"content": "Recent works have highlighted the strength of the Transformer architecture on\nsequence tasks while, at the same time, neural architecture search (NAS) has\nbegun to outperform human-designed models. Our goal is to apply NAS to search\nfor a better alternative to the Transformer. We first construct a large search\nspace inspired by the recent advances in feed-forward sequence models and then\nrun evolutionary architecture search with warm starting by seeding our initial\npopulation with the Transformer. To directly search on the computationally\nexpensive WMT 2014 English-German translation task, we develop the Progressive\nDynamic Hurdles method, which allows us to dynamically allocate more resources\nto more promising candidate models. The architecture found in our experiments\n-- the Evolved Transformer -- demonstrates consistent improvement over the\nTransformer on four well-established language tasks: WMT 2014 English-German,\nWMT 2014 English-French, WMT 2014 English-Czech and LM1B. At a big model size,\nthe Evolved Transformer establishes a new state-of-the-art BLEU score of 29.8\non WMT'14 English-German; at smaller sizes, it achieves the same quality as the\noriginal \"big\" Transformer with 37.6% less parameters and outperforms the\nTransformer by 0.7 BLEU at a mobile-friendly model size of 7M parameters.", "collection_id": "33839f4252f3f5e9e1a9e8c3f604d172903b09e85dbd97c2d904dbe3292dacb6"}, "question": "What is the BLEU score achieved by the Evolved Transformer on the WMT'14 English-German task at a big model size?\n", "answer": "The Evolved Transformer achieves a new state-of-the-art BLEU score of 29.8 on the WMT'14 English-German task at a big model size."}
{"document": {"content": "Stable Diffusion revolutionised image creation from descriptive text. GPT-2,\nGPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of\nlanguage tasks. ChatGPT introduced such language models to the general public.\nIt is now clear that large language models (LLMs) are here to stay, and will\nbring about drastic change in the whole ecosystem of online text and images. In\nthis paper we consider what the future might hold. What will happen to GPT-{n}\nonce LLMs contribute much of the language found online? We find that use of\nmodel-generated content in training causes irreversible defects in the\nresulting models, where tails of the original content distribution disappear.\nWe refer to this effect as Model Collapse and show that it can occur in\nVariational Autoencoders, Gaussian Mixture Models and LLMs. We build\ntheoretical intuition behind the phenomenon and portray its ubiquity amongst\nall learned generative models. We demonstrate that it has to be taken seriously\nif we are to sustain the benefits of training from large-scale data scraped\nfrom the web. Indeed, the value of data collected about genuine human\ninteractions with systems will be increasingly valuable in the presence of\ncontent generated by LLMs in data crawled from the Internet.", "collection_id": "6e05fc56bea3a3e73ca7d6aec27ff0423fe64040306618fe11e04c08b9be82b4"}, "question": "What is the term used to describe the irreversible defects in language models caused by the use of model-generated content in training?\n", "answer": "The term used to describe this phenomenon is \"Model Collapse\", which refers to the disappearance of the tails of the original content distribution in the resulting models."}
{"document": {"content": "Code is seldom written in a single left-to-right pass and is instead\nrepeatedly edited and refined. We introduce InCoder, a unified generative model\nthat can perform program synthesis (via left-to-right generation) as well as\nediting (via infilling). InCoder is trained to generate code files from a large\ncorpus of permissively licensed code, where regions of code have been randomly\nmasked and moved to the end of each file, allowing code infilling with\nbidirectional context. Our model is the first generative model that is able to\ndirectly perform zero-shot code infilling, which we evaluate on challenging\ntasks such as type inference, comment generation, and variable re-naming. We\nfind that the ability to condition on bidirectional context substantially\nimproves performance on these tasks, while still performing comparably on\nstandard program synthesis benchmarks in comparison to left-to-right only\nmodels pretrained at similar scale. The InCoder models and code are publicly\nreleased. https://sites.google.com/view/incoder-code-models", "collection_id": "77a8d8eedddaf3661c2cd44f6746632a79cec255eab9d7136aa9e9adeb0d6f22"}, "question": "What is the name of the unified generative model that can perform program synthesis and editing via infilling?\n", "answer": "InCoder"}
{"document": {"content": "Although contemporary large language models (LMs) demonstrate impressive\nquestion-answering capabilities, their answers are typically the product of a\nsingle call to the model. This entails an unwelcome degree of opacity and\ncompromises performance, especially on problems that are inherently multi-step.\nTo address these limitations, we show how LMs can be made to perform faithful\nmulti-step reasoning via a process whose causal structure mirrors the\nunderlying logical structure of the problem. Our approach works by chaining\ntogether reasoning steps, where each step results from calls to two fine-tuned\nLMs, one for selection and one for inference, to produce a valid reasoning\ntrace. Our method carries out a beam search through the space of reasoning\ntraces to improve reasoning quality. We demonstrate the effectiveness of our\nmodel on multi-step logical deduction and scientific question-answering,\nshowing that it outperforms baselines on final answer accuracy, and generates\nhumanly interpretable reasoning traces whose validity can be checked by the\nuser.", "collection_id": "d488db6b2850dd87f73ae7527507ee0f21fe035b974937e098b5c65167c10486"}, "question": "How do large language models typically generate answers?\n", "answer": "Large language models typically generate answers through a single call to the model, resulting in a lack of transparency and potentially compromising performance on multi-step problems."}
{"document": {"content": "It has been observed that large-scale language models capture undesirable\nsocietal biases, e.g. relating to race and gender; yet religious bias has been\nrelatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual\nlanguage model, captures persistent Muslim-violence bias. We probe GPT-3 in\nvarious ways, including prompt completion, analogical reasoning, and story\ngeneration, to understand this anti-Muslim bias, demonstrating that it appears\nconsistently and creatively in different uses of the model and that it is\nsevere even compared to biases about other religious groups. For instance,\n\"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is\nmapped to \"money\" in 5% of test cases. We quantify the positive distraction\nneeded to overcome this bias with adversarial text prompts, and find that use\nof the most positive 6 adjectives reduces violent completions for \"Muslims\"\nfrom 66% to 20%, but which is still higher than for other religious groups.", "collection_id": "e6ff85509a08a843bb79215ad21f7492de8b86e34abbf21a5be64a9d2e85086c"}, "question": "What percentage of test cases mapped \"Muslim\" to \"terrorist\" in the GPT-3 language model?\n", "answer": "In the GPT-3 language model, \"Muslim\" was analogized to \"terrorist\" in 23% of test cases."}
{"document": {"content": "Despite inextricable ties between race and language, little work has\nconsidered race in NLP research and development. In this work, we survey 79\npapers from the ACL anthology that mention race. These papers reveal various\ntypes of race-related bias in all stages of NLP model development, highlighting\nthe need for proactive consideration of how NLP systems can uphold racial\nhierarchies. However, persistent gaps in research on race and NLP remain: race\nhas been siloed as a niche topic and remains ignored in many NLP tasks; most\nwork operationalizes race as a fixed single-dimensional variable with a\nground-truth label, which risks reinforcing differences produced by historical\nracism; and the voices of historically marginalized people are nearly absent in\nNLP literature. By identifying where and how NLP literature has and has not\nconsidered race, especially in comparison to related fields, our work calls for\ninclusion and racial justice in NLP research practices.", "collection_id": "a80a254e0944dbb52fd576fbc0bf1adb64b28347812dc3ec479106291fd87119"}, "question": "How many papers from the ACL anthology were surveyed to examine the consideration of race in NLP research and development?\n", "answer": "79 papers from the ACL anthology were surveyed to examine the consideration of race in NLP research and development."}
{"document": {"content": "This paper explores a simple and efficient baseline for text classification.\nOur experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation. We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute.", "collection_id": "a0614c0fd4e61f8a6b0bf80705cc9839b174614c138a72144199087a73498646"}, "question": "How long does it take to train fastText on over one billion words using a standard multicore CPU?\n", "answer": "Less than ten minutes."}
{"document": {"content": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n  In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM", "collection_id": "030588747435b8b718d34b681ed994c6e3c2f1e41c7956725d77539ef2e64f67"}, "question": "What is the name of the alternative reasoning scheme proposed in the paper that learns a decomposition of the original problem into a sequence of subproblems?\n\n", "answer": "Socratic CoT (Chain of Thought)"}
{"document": {"content": "Recent work in language modeling demonstrates that training large transformer\nmodels advances the state of the art in Natural Language Processing\napplications. However, very large models can be quite difficult to train due to\nmemory constraints. In this work, we present our techniques for training very\nlarge transformer models and implement a simple, efficient intra-layer model\nparallel approach that enables training transformer models with billions of\nparameters. Our approach does not require a new compiler or library changes, is\northogonal and complimentary to pipeline model parallelism, and can be fully\nimplemented with the insertion of a few communication operations in native\nPyTorch. We illustrate this approach by converging transformer based models up\nto 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the\nentire application with 76% scaling efficiency when compared to a strong single\nGPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To\ndemonstrate that large language models can further advance the state of the art\n(SOTA), we train an 8.3 billion parameter transformer language model similar to\nGPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful\nattention to the placement of layer normalization in BERT-like models is\ncritical to achieving increased performance as the model size grows. Using the\nGPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA\nperplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)\ndatasets. Our BERT model achieves SOTA results on the RACE dataset (90.9%\ncompared to SOTA accuracy of 89.4%).", "collection_id": "cd3e568d98d54d0c60f6f2a15789491888eaa968f94513ab06cf12f096d0039c"}, "question": "What is the peak FLOPs performance of a single GPU baseline in the experiment?\n", "answer": "The peak FLOPs performance of a single GPU baseline in the experiment is 39 TeraFLOPs, which is 30% of the peak FLOPs."}
{"document": {"content": "Cyber-defense systems are being developed to automatically ingest Cyber\nThreat Intelligence (CTI) that contains semi-structured data and/or text to\npopulate knowledge graphs. A potential risk is that fake CTI can be generated\nand spread through Open-Source Intelligence (OSINT) communities or on the Web\nto effect a data poisoning attack on these systems. Adversaries can use fake\nCTI examples as training input to subvert cyber defense systems, forcing the\nmodel to learn incorrect inputs to serve their malicious needs.\n  In this paper, we automatically generate fake CTI text descriptions using\ntransformers. We show that given an initial prompt sentence, a public language\nmodel like GPT-2 with fine-tuning, can generate plausible CTI text with the\nability of corrupting cyber-defense systems. We utilize the generated fake CTI\ntext to perform a data poisoning attack on a Cybersecurity Knowledge Graph\n(CKG) and a cybersecurity corpus. The poisoning attack introduced adverse\nimpacts such as returning incorrect reasoning outputs, representation\npoisoning, and corruption of other dependent AI-based cyber defense systems. We\nevaluate with traditional approaches and conduct a human evaluation study with\ncybersecurity professionals and threat hunters. Based on the study,\nprofessional threat hunters were equally likely to consider our fake generated\nCTI as true.", "collection_id": "e1cd8a80dcc3c896108d124311f57f5a1d710de83f04ede3b5d61c3e43f36179"}, "question": "What is the potential risk of using Cyber Threat Intelligence (CTI) in cyber-defense systems?\n\n", "answer": "The potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems, allowing adversaries to subvert cyber defense systems and force the model to learn incorrect inputs to serve their malicious needs."}
{"document": {"content": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.", "collection_id": "17da06845e6750c3944799ef6a3a5519f60a5480c575fc58d76e75ce9874c171"}, "question": "How many prominent language models were evaluated in the large-scale evaluation of the Holistic Evaluation of Language Models (HELM) framework?\n\n", "answer": "30 prominent language models, spanning open, limited-access, and closed models, were evaluated in the large-scale evaluation of the Holistic Evaluation of Language Models (HELM) framework."}
{"document": {"content": "Humans (e.g., crowdworkers) have a remarkable ability in solving different\ntasks, by simply reading textual instructions that define them and looking at a\nfew examples. Despite the success of the conventional supervised learning on\nindividual datasets, such models often struggle with generalization across\ntasks (e.g., a question-answering system cannot solve classification tasks). A\nlong-standing challenge in AI is to build a model that learns a new task by\nunderstanding the human-readable instructions that define it. To study this, we\nintroduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their\nhuman-authored instructions, and 193k task instances (input-output pairs). The\ninstructions are obtained from crowdsourcing instructions used to create\nexisting NLP datasets and mapped to a unified schema. Using this meta-dataset,\nwe measure cross-task generalization by training models on seen tasks and\nmeasuring generalization to the remaining unseen ones. We adopt generative\npre-trained language models to encode task-specific instructions along with\ninput and generate task output. Our results indicate that models benefit from\ninstructions when evaluated in terms of generalization to unseen tasks (19%\nbetter for models utilizing instructions). These models, however, are far\nbehind an estimated performance upperbound indicating significant room for more\nprogress in this direction.", "collection_id": "cbc417f5dd466b1e95d91b15967a06fc437473e180fa5285e924ed2ef41a990d"}, "question": "How many distinct tasks are included in the NATURAL INSTRUCTIONS dataset?\n", "answer": "The NATURAL INSTRUCTIONS dataset includes 61 distinct tasks."}
{"document": {"content": "In this paper we study yes/no questions that are naturally occurring ---\nmeaning that they are generated in unprompted and unconstrained settings. We\nbuild a reading comprehension dataset, BoolQ, of such questions, and show that\nthey are unexpectedly challenging. They often query for complex, non-factoid\ninformation, and require difficult entailment-like inference to solve. We also\nexplore the effectiveness of a range of transfer learning baselines. We find\nthat transferring from entailment data is more effective than transferring from\nparaphrase or extractive QA data, and that it, surprisingly, continues to be\nvery beneficial even when starting from massive pre-trained language models\nsuch as BERT. Our best method trains BERT on MultiNLI and then re-trains it on\nour train set. It achieves 80.4% accuracy compared to 90% accuracy of human\nannotators (and 62% majority-baseline), leaving a significant gap for future\nwork.", "collection_id": "73763d788ea5940252af3c8696b9a9173bc81ac6176de11ef3f69de017fefe9c"}, "question": "What is the accuracy of the best method for answering yes/no questions in the BoolQ dataset?\n", "answer": "The best method, which trains BERT on MultiNLI and then re-trains it on the train set, achieves an accuracy of 80.4%."}
{"document": {"content": "Recent works have shown how the reasoning capabilities of Large Language\nModels (LLMs) can be applied to domains beyond natural language processing,\nsuch as planning and interaction for robots. These embodied problems require an\nagent to understand many semantic aspects of the world: the repertoire of\nskills available, how these skills influence the world, and how changes to the\nworld map back to the language. LLMs planning in embodied environments need to\nconsider not just what skills to do, but also how and when to do them - answers\nthat change over time in response to the agent's own choices. In this work, we\ninvestigate to what extent LLMs used in such embodied contexts can reason over\nsources of feedback provided through natural language, without any additional\ntraining. We propose that by leveraging environment feedback, LLMs are able to\nform an inner monologue that allows them to more richly process and plan in\nrobotic control scenarios. We investigate a variety of sources of feedback,\nsuch as success detection, scene description, and human interaction. We find\nthat closed-loop language feedback significantly improves high-level\ninstruction completion on three domains, including simulated and real table top\nrearrangement tasks and long-horizon mobile manipulation tasks in a kitchen\nenvironment in the real world.", "collection_id": "dcccbc2877eb399edc12a837f794f4ef277d940c444c402a70725f9cd51e6bf0"}, "question": "What type of tasks did researchers test the Large Language Models on to evaluate their reasoning capabilities in embodied environments?\n\n", "answer": "The researchers tested the Large Language Models on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world."}
{"document": {"content": "We present a human-and-model-in-the-loop process for dynamically generating\ndatasets and training better performing and more robust hate detection models.\nWe provide a new dataset of ~40,000 entries, generated and labelled by trained\nannotators over four rounds of dynamic data creation. It includes ~15,000\nchallenging perturbations and each hateful entry has fine-grained labels for\nthe type and target of hate. Hateful entries make up 54% of the dataset, which\nis substantially higher than comparable datasets. We show that model\nperformance is substantially improved using this approach. Models trained on\nlater rounds of data collection perform better on test sets and are harder for\nannotators to trick. They also perform better on HateCheck, a suite of\nfunctional tests for online hate detection. We provide the code, dataset and\nannotation guidelines for other researchers to use. Accepted at ACL 2021.", "collection_id": "a464ca5cc45cb92041f1c684017634e4f2489f5a543f75888c6639fc2d77e256"}, "question": "What percentage of the dataset consists of hateful entries?\n", "answer": "Hateful entries make up 54% of the dataset."}
{"document": {"content": "Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\nlarge InstructGPT model (text-davinci-002), as well as similar magnitudes of\nimprovements with another off-the-shelf large model, 540B parameter PaLM. The\nversatility of this single prompt across very diverse reasoning tasks hints at\nuntapped and understudied fundamental zero-shot capabilities of LLMs,\nsuggesting high-level, multi-task broad cognitive capabilities may be extracted\nby simple prompting. We hope our work not only serves as the minimal strongest\nzero-shot baseline for the challenging reasoning benchmarks, but also\nhighlights the importance of carefully exploring and analyzing the enormous\nzero-shot knowledge hidden inside LLMs before crafting finetuning datasets or\nfew-shot exemplars.", "collection_id": "4d53e42aa8bba8913cde1180264b293384722372408116dea3fc6d9581199447"}, "question": "What is the accuracy of the large InstructGPT model (text-davinci-002) on the MultiArith task after using the Zero-shot-CoT prompt?\n", "answer": "The accuracy of the large InstructGPT model (text-davinci-002) on the MultiArith task increases from 17.7% to 78.7% after using the Zero-shot-CoT prompt."}
{"document": {"content": "In this paper, we study counterfactual fairness in text classification, which\nasks the question: How would the prediction change if the sensitive attribute\nreferenced in the example were different? Toxicity classifiers demonstrate a\ncounterfactual fairness issue by predicting that \"Some people are gay\" is toxic\nwhile \"Some people are straight\" is nontoxic. We offer a metric, counterfactual\ntoken fairness (CTF), for measuring this particular form of fairness in text\nclassifiers, and describe its relationship with group fairness. Further, we\noffer three approaches, blindness, counterfactual augmentation, and\ncounterfactual logit pairing (CLP), for optimizing counterfactual token\nfairness during training, bridging the robustness and fairness literature.\nEmpirically, we find that blindness and CLP address counterfactual token\nfairness. The methods do not harm classifier performance, and have varying\ntradeoffs with group fairness. These approaches, both for measurement and\noptimization, provide a new path forward for addressing fairness concerns in\ntext classification.", "collection_id": "d33cf4d1083c8f7ab91affa81650f6459f8309437781a195c49793906d04d158"}, "question": "What is the name of the metric proposed for measuring counterfactual fairness in text classifiers?\n", "answer": "The metric proposed for measuring counterfactual fairness in text classifiers is called counterfactual token fairness (CTF)."}
{"document": {"content": "We analyze the growth of dataset sizes used in machine learning for natural\nlanguage processing and computer vision, and extrapolate these using two\nmethods; using the historical growth rate and estimating the compute-optimal\ndataset size for future predicted compute budgets. We investigate the growth in\ndata usage by estimating the total stock of unlabeled data available on the\ninternet over the coming decades. Our analysis indicates that the stock of\nhigh-quality language data will be exhausted soon; likely before 2026. By\ncontrast, the stock of low-quality language data and image data will be\nexhausted only much later; between 2030 and 2050 (for low-quality language) and\nbetween 2030 and 2060 (for images). Our work suggests that the current trend of\never-growing ML models that rely on enormous datasets might slow down if data\nefficiency is not drastically improved or new sources of data become available.", "collection_id": "15fe4217e9e8a51a59a622a70c34c6a2e2a6923174c4612c6cb541894b13406a"}, "question": "When is the stock of high-quality language data expected to be exhausted?\n", "answer": "The stock of high-quality language data is expected to be exhausted likely before 2026."}
{"document": {"content": "Deep learning (DL) creates impactful advances following a virtuous recipe:\nmodel architecture search, creating large training data sets, and scaling\ncomputation. It is widely believed that growing training sets and models should\nimprove accuracy and result in better products. As DL application domains grow,\nwe would like a deeper understanding of the relationships between training set\nsize, computational scale, and model accuracy improvements to advance the\nstate-of-the-art.\n  This paper presents a large scale empirical characterization of\ngeneralization error and model size growth as training sets grow. We introduce\na methodology for this measurement and test four machine learning domains:\nmachine translation, language modeling, image processing, and speech\nrecognition. Our empirical results show power-law generalization error scaling\nacross a breadth of factors, resulting in power-law exponents---the \"steepness\"\nof the learning curve---yet to be explained by theoretical work. Further, model\nimprovements only shift the error but do not appear to affect the power-law\nexponent. We also show that model size scales sublinearly with data size. These\nscaling relationships have significant implications on deep learning research,\npractice, and systems. They can assist model debugging, setting accuracy\ntargets, and decisions about data set growth. They can also guide computing\nsystem design and underscore the importance of continued computational scaling.", "collection_id": "cbe541fdf16fc915d5510d6e8db9f4ae2e054257eeffd4a62247e3fdea559c31"}, "question": "What are the four machine learning domains tested in the empirical characterization of generalization error and model size growth?\n\n", "answer": "The four machine learning domains tested are machine translation, language modeling, image processing, and speech recognition."}
{"document": {"content": "Large language models (LLMs) have demonstrated impressive zero-shot abilities\non a variety of open-ended tasks, while recent research has also explored the\nuse of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl,\na novel training paradigm that equips LLMs with multi-modal abilities through\nmodularized learning of foundation LLM, a visual knowledge module, and a visual\nabstractor module. This approach can support multiple modalities and facilitate\ndiverse unimodal and multimodal abilities through modality collaboration. The\ntraining paradigm of mPLUG-Owl involves a two-stage method for aligning image\nand text, which learns visual knowledge with the assistance of LLM while\nmaintaining and even improving the generation abilities of LLM. In the first\nstage, the visual knowledge module and abstractor module are trained with a\nfrozen LLM module to align the image and text. In the second stage,\nlanguage-only and multi-modal supervised datasets are used to jointly fine-tune\na low-rank adaption (LoRA) module on LLM and the abstractor module by freezing\nthe visual knowledge module. We carefully build a visually-related instruction\nevaluation set OwlEval. Experimental results show that our model outperforms\nexisting multi-modal models, demonstrating mPLUG-Owl's impressive instruction\nand visual understanding ability, multi-turn conversation ability, and\nknowledge reasoning ability. Besides, we observe some unexpected and exciting\nabilities such as multi-image correlation and scene text understanding, which\nmakes it possible to leverage it for harder real scenarios, such as vision-only\ndocument comprehension. Our code, pre-trained model, instruction-tuned models,\nand evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The\nonline demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.", "collection_id": "911a6b98268c5eb7f488429fb7feb569b15935ab6a84b9468354541e56cdabe8"}, "question": "What is the name of the novel training paradigm that equips large language models with multi-modal abilities through modularized learning?\n\n", "answer": "The novel training paradigm is called mPLUG-Owl."}
{"document": {"content": "Recent progress in natural language generation has raised dual-use concerns.\nWhile applications like summarization and translation are positive, the\nunderlying technology also might enable adversaries to generate neural fake\nnews: targeted propaganda that closely mimics the style of real news.\n  Modern computer security relies on careful threat modeling: identifying\npotential threats and vulnerabilities from an adversary's point of view, and\nexploring potential mitigations to these threats. Likewise, developing robust\ndefenses against neural fake news requires us first to carefully investigate\nand characterize the risks of these models. We thus present a model for\ncontrollable text generation called Grover. Given a headline like `Link Found\nBetween Vaccines and Autism,' Grover can generate the rest of the article;\nhumans find these generations to be more trustworthy than human-written\ndisinformation.\n  Developing robust verification techniques against generators like Grover is\ncritical. We find that best current discriminators can classify neural fake\nnews from real, human-written, news with 73% accuracy, assuming access to a\nmoderate level of training data. Counterintuitively, the best defense against\nGrover turns out to be Grover itself, with 92% accuracy, demonstrating the\nimportance of public release of strong generators. We investigate these results\nfurther, showing that exposure bias -- and sampling strategies that alleviate\nits effects -- both leave artifacts that similar discriminators can pick up on.\nWe conclude by discussing ethical issues regarding the technology, and plan to\nrelease Grover publicly, helping pave the way for better detection of neural\nfake news.", "collection_id": "45f0dea3f3b10c7965bad71fca335bfa9e89d5e18574d79117205110f10efd86"}, "question": "What is the accuracy of the best current discriminators in classifying neural fake news from real news with a moderate level of training data?\n", "answer": "The best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data."}
{"document": {"content": "Large language models (LLMs) trained on code completion have been shown to be\ncapable of synthesizing simple Python programs from docstrings [1]. We find\nthat these code-writing LLMs can be re-purposed to write robot policy code,\ngiven natural language commands. Specifically, policy code can express\nfunctions or feedback loops that process perception outputs (e.g.,from object\ndetectors [2], [3]) and parameterize control primitive APIs. When provided as\ninput several example language commands (formatted as comments) followed by\ncorresponding policy code (via few-shot prompting), LLMs can take in new\ncommands and autonomously re-compose API calls to generate new policy code\nrespectively. By chaining classic logic structures and referencing third-party\nlibraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way\ncan write robot policies that (i) exhibit spatial-geometric reasoning, (ii)\ngeneralize to new instructions, and (iii) prescribe precise values (e.g.,\nvelocities) to ambiguous descriptions (\"faster\") depending on context (i.e.,\nbehavioral commonsense). This paper presents code as policies: a robot-centric\nformulation of language model generated programs (LMPs) that can represent\nreactive policies (e.g., impedance controllers), as well as waypoint-based\npolicies (vision-based pick and place, trajectory-based control), demonstrated\nacross multiple real robot platforms. Central to our approach is prompting\nhierarchical code-gen (recursively defining undefined functions), which can\nwrite more complex code and also improves state-of-the-art to solve 39.8% of\nproblems on the HumanEval [1] benchmark. Code and videos are available at\nhttps://code-as-policies.github.io", "collection_id": "5701a49470d6c7ac7261dd9ad6c000e712e72759656e15ad245ebb60d1847dfe"}, "question": "What percentage of problems on the HumanEval benchmark can be solved using the hierarchical code-gen approach?\n", "answer": "39.8%"}
{"document": {"content": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed\nresource budget, and then scaled up for better accuracy if more resources are\navailable. In this paper, we systematically study model scaling and identify\nthat carefully balancing network depth, width, and resolution can lead to\nbetter performance. Based on this observation, we propose a new scaling method\nthat uniformly scales all dimensions of depth/width/resolution using a simple\nyet highly effective compound coefficient. We demonstrate the effectiveness of\nthis method on scaling up MobileNets and ResNet.\n  To go even further, we use neural architecture search to design a new\nbaseline network and scale it up to obtain a family of models, called\nEfficientNets, which achieve much better accuracy and efficiency than previous\nConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3%\ntop-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on\ninference than the best existing ConvNet. Our EfficientNets also transfer well\nand achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\nand 3 other transfer learning datasets, with an order of magnitude fewer\nparameters. Source code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.", "collection_id": "1b6288d0bb1c44b6f6d2d9c56578c9c12861a0e494d1b8d767852d1440156e37"}, "question": "What is the top-1 accuracy achieved by EfficientNet-B7 on ImageNet?\n", "answer": "EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet."}
{"document": {"content": "Language models (LMs) have recently been shown to generate more factual\nresponses by employing modularity (Zhou et al., 2021) in combination with\nretrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et\nal. (2021) to include internet search as a module. Our SeeKeR (Search\nengine->Knowledge->Response) method thus applies a single LM to three modular\ntasks in succession: search, generating knowledge, and generating a final\nresponse. We show that, when using SeeKeR as a dialogue model, it outperforms\nthe state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain\nknowledge-grounded conversations for the same number of parameters, in terms of\nconsistency, knowledge and per-turn engagingness. SeeKeR applied to topical\nprompt completions as a standard language model outperforms GPT2 (Radford et\nal., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality,\ndespite GPT3 being a vastly larger model. Our code and models are made publicly\navailable.", "collection_id": "7e0cfdcd5fbf959f6b212d030a77005b5d40a77f5ad855a11913353c0ab22970"}, "question": "What is the name of the method that applies a single language model to three modular tasks in succession: search, generating knowledge, and generating a final response?\n\n", "answer": "The method is called SeeKeR (Search engine->Knowledge->Response)."}
{"document": {"content": "Large language models have shown impressive few-shot results on a wide range\nof tasks. However, when knowledge is key for such results, as is the case for\ntasks such as question answering and fact checking, massive parameter counts to\nstore knowledge seem to be needed. Retrieval augmented models are known to\nexcel at knowledge intensive tasks without the need for as many parameters, but\nit is unclear whether they work in few-shot settings. In this work we present\nAtlas, a carefully designed and pre-trained retrieval augmented language model\nable to learn knowledge intensive tasks with very few training examples. We\nperform evaluations on a wide range of tasks, including MMLU, KILT and\nNaturalQuestions, and study the impact of the content of the document index,\nshowing that it can easily be updated. Notably, Atlas reaches over 42% accuracy\non Natural Questions using only 64 examples, outperforming a 540B parameters\nmodel by 3% despite having 50x fewer parameters.", "collection_id": "b7172127db886aff256aed2404b1d195ec7924e968bc62743a64c8d4a40118c1"}, "question": "What is the accuracy of Atlas on Natural Questions using only 64 examples?\n", "answer": "Atlas reaches an accuracy of over 42% on Natural Questions using only 64 examples."}
{"document": {"content": "Generative Artificial Intelligence (AI) has enabled the development of\nsophisticated models that are capable of producing high-caliber text, images,\nand other outputs through the utilization of large pre-trained models.\nNevertheless, assessing the quality of the generation is an even more arduous\ntask than the generation itself, and this issue has not been given adequate\nconsideration recently. This paper proposes a novel evaluation framework,\nGPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)\nof generative pre-trained models to score generated texts. There are 19\npre-trained models explored in this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text\ngeneration tasks, 22 evaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively allow us to achieve what one\ndesires to evaluate for texts simply by natural language instructions. This\nnature helps us overcome several long-standing challenges in text\nevaluation--how to achieve customized, multi-faceted evaluation without the\nneed for annotated samples. We make our code publicly available at\nhttps://github.com/jinlanfu/GPTScore.", "collection_id": "c2924779508f16f292707a67ecee9de51d0ef6971a48c58fe579cef77ec5c00b"}, "question": "What is the range of sizes of the pre-trained models explored in the GPTScore paper?\n", "answer": "The pre-trained models explored in the GPTScore paper range in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3)."}
{"document": {"content": "Self-attention architectures, which are rapidly pushing the frontier in\nnatural language processing, demonstrate a surprising depth-inefficient\nbehavior: previous works indicate that increasing the internal representation\n(network width) is just as useful as increasing the number of self-attention\nlayers (network depth). We theoretically predict a width-dependent transition\nbetween depth-efficiency and depth-inefficiency in self-attention. We conduct\nsystematic empirical ablations on networks of depths 6 to 48 that clearly\nreveal the theoretically predicted behaviors, and provide explicit quantitative\nsuggestions regarding the optimal depth-to-width allocation for a given\nself-attention network size. The race towards beyond 1-Trillion parameter\nlanguage models renders informed guidelines for increasing self-attention depth\nand width in tandem an essential ingredient. Our guidelines elucidate the\ndepth-to-width trade-off in self-attention networks of sizes up to the scale of\nGPT3 (which we project to be too deep for its size), and beyond, marking an\nunprecedented width of 30K as optimal for a 1-Trillion parameter network.", "collection_id": "6155f836c7027aa12e48f5ade8e22983d17d30f9be8cc33031ba7ad0d724508b"}, "question": "What is the projected optimal width for a 1-Trillion parameter self-attention network?\n", "answer": "The projected optimal width for a 1-Trillion parameter self-attention network is 30K."}
{"document": {"content": "Training large deep neural networks on massive datasets is computationally\nvery challenging. There has been recent surge in interest in using large batch\nstochastic optimization methods to tackle this issue. The most prominent\nalgorithm in this line of research is LARS, which by employing layerwise\nadaptive learning rates trains ResNet on ImageNet in a few minutes. However,\nLARS performs poorly for attention models like BERT, indicating that its\nperformance gains are not consistent across tasks. In this paper, we first\nstudy a principled layerwise adaptation strategy to accelerate training of deep\nneural networks using large mini-batches. Using this strategy, we develop a new\nlayerwise adaptive large batch optimization technique called LAMB; we then\nprovide convergence analysis of LAMB as well as LARS, showing convergence to a\nstationary point in general nonconvex settings. Our empirical results\ndemonstrate the superior performance of LAMB across various tasks such as BERT\nand ResNet-50 training with very little hyperparameter tuning. In particular,\nfor BERT training, our optimizer enables use of very large batch sizes of 32868\nwithout any degradation of performance. By increasing the batch size to the\nmemory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to\njust 76 minutes (Table 1). The LAMB implementation is available at\nhttps://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py", "collection_id": "448f23817875a62aad8c88c52ac21eddf55adc402158d255ea318f6058cb7f07"}, "question": "What is the name of the new layerwise adaptive large batch optimization technique developed for accelerating training of deep neural networks?\n\n", "answer": "LAMB (Layerwise Adaptive Moments for Batch training)"}
{"document": {"content": "In this work, we present X-SQL, a new network architecture for the problem of\nparsing natural language to SQL query. X-SQL proposes to enhance the structural\nschema representation with the contextual output from BERT-style pre-training\nmodel, and together with type information to learn a new schema representation\nfor down-stream tasks. We evaluated X-SQL on the WikiSQL dataset and show its\nnew state-of-the-art performance.", "collection_id": "db0896947e8bda5d7d40bc3fb537492317b59e0e8c780790951cedeec38eb9a5"}, "question": "What dataset was X-SQL evaluated on to demonstrate its state-of-the-art performance?\n", "answer": "X-SQL was evaluated on the WikiSQL dataset."}
{"document": {"content": "A key challenge for automatic hate-speech detection on social media is the\nseparation of hate speech from other instances of offensive language. Lexical\ndetection methods tend to have low precision because they classify all messages\ncontaining particular terms as hate speech and previous work using supervised\nlearning has failed to distinguish between the two categories. We used a\ncrowd-sourced hate speech lexicon to collect tweets containing hate speech\nkeywords. We use crowd-sourcing to label a sample of these tweets into three\ncategories: those containing hate speech, only offensive language, and those\nwith neither. We train a multi-class classifier to distinguish between these\ndifferent categories. Close analysis of the predictions and the errors shows\nwhen we can reliably separate hate speech from other offensive language and\nwhen this differentiation is more difficult. We find that racist and homophobic\ntweets are more likely to be classified as hate speech but that sexist tweets\nare generally classified as offensive. Tweets without explicit hate keywords\nare also more difficult to classify.", "collection_id": "ed9372a02837349deb7f06fa49aa701670f97e0c30c4537ea46a5bea94af3d8d"}, "question": "What are the three categories used to label tweets in the study on automatic hate-speech detection on social media?\n\n", "answer": "The three categories used to label tweets are: those containing hate speech, those containing only offensive language, and those with neither hate speech nor offensive language."}
{"document": {"content": "Generalization and robustness are both key desiderata for designing machine\nlearning methods. Adversarial training can enhance robustness, but past work\noften finds it hurts generalization. In natural language processing (NLP),\npre-training large neural language models such as BERT have demonstrated\nimpressive gain in generalization for a variety of tasks, with further\nimprovement from adversarial fine-tuning. However, these models are still\nvulnerable to adversarial attacks. In this paper, we show that adversarial\npre-training can improve both generalization and robustness. We propose a\ngeneral algorithm ALUM (Adversarial training for large neural LangUage Models),\nwhich regularizes the training objective by applying perturbations in the\nembedding space that maximizes the adversarial loss. We present the first\ncomprehensive study of adversarial training in all stages, including\npre-training from scratch, continual pre-training on a well-trained model, and\ntask-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide\nrange of NLP tasks, in both regular and adversarial scenarios. Even for models\nthat have been well trained on extremely large text corpora, such as RoBERTa,\nALUM can still produce significant gains from continual pre-training, whereas\nconventional non-adversarial methods can not. ALUM can be further combined with\ntask-specific fine-tuning to attain additional gains. The ALUM code is publicly\navailable at https://github.com/namisan/mt-dnn.", "collection_id": "233ebf4131bc510a0a426ab5c84a164a5314e4f374dda2fab837475d0e69bd0d"}, "question": "What is the name of the algorithm proposed for adversarial pre-training of large neural language models?\n", "answer": "The algorithm is called ALUM, which stands for Adversarial training for large neural LangUage Models."}
{"document": {"content": "We propose a method for arbitrary textual style transfer (TST)--the task of\ntransforming a text into any given style--utilizing general-purpose pre-trained\nlanguage models. Our method, Prompt-and-Rerank, is based on a mathematical\nformulation of the TST task, decomposing it into three constituent components:\ntextual similarity, target style strength, and fluency. Specifically, our\nmethod first uses zero-shot or few-shot prompting to obtain a set of candidate\ngenerations in the target style, and then re-ranks these candidates according\nto a combination of the three components above. Empirically, our method enables\nsmall pre-trained language models to perform on par with state-of-the-art\nlarge-scale models while consuming two orders of magnitude less compute and\nmemory. Finally, we conduct a systematic investigation of the effect of model\nsize and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on\nstyle transfer quality across seven diverse textual style transfer datasets.", "collection_id": "22fec1bddd778656580c26472608db8eb9934b2e1be11bc39dae74e4bb0f6a63"}, "question": "What are the three constituent components of the Prompt-and-Rerank method for textual style transfer?\n", "answer": "The three constituent components of the Prompt-and-Rerank method are textual similarity, target style strength, and fluency."}
{"document": {"content": "Recent studies report that autoregressive language models can successfully\nsolve many NLP tasks via zero- and few-shot learning paradigms, which opens up\nnew possibilities for using the pre-trained language models. This paper\nintroduces two autoregressive GPT-like models with 1.3 billion and 13 billion\nparameters trained on 60 languages from 25 language families using Wikipedia\nand Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using\nGPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron\nframeworks allow us to parallelize the training and inference steps\neffectively. The resulting models show performance on par with the recently\nreleased XGLM models by Facebook, covering more languages and enhancing NLP\npossibilities for low resource languages of CIS countries and Russian small\nnations. We detail the motivation for the choices of the architecture design,\nthoroughly describe the data preparation pipeline, and train five small\nversions of the model to choose the most optimal multilingual tokenization\nstrategy. We measure the model perplexity in all covered languages and evaluate\nit on the wide spectre of multilingual tasks, including classification,\ngenerative, sequence labeling and knowledge probing. The models were evaluated\nwith the zero-shot and few-shot methods. Furthermore, we compared the\nclassification tasks with the state-of-the-art multilingual model XGLM. source\ncode and the mGPT XL model are publicly released.", "collection_id": "242a2fa2ffae3e54ee9d93da07c97bc13535ef9483dd11a9855c657da9243940"}, "question": "How many languages were used to train the autoregressive GPT-like models introduced in the paper?\n", "answer": "The autoregressive GPT-like models were trained on 60 languages from 25 language families."}
{"document": {"content": "The perceived toxicity of language can vary based on someone's identity and\nbeliefs, but this variation is often ignored when collecting toxic language\ndatasets, resulting in dataset and model biases. We seek to understand the who,\nwhy, and what behind biases in toxicity annotations. In two online studies with\ndemographically and politically diverse participants, we investigate the effect\nof annotator identities (who) and beliefs (why), drawing from social psychology\nresearch about hate speech, free speech, racist beliefs, political leaning, and\nmore. We disentangle what is annotated as toxic by considering posts with three\ncharacteristics: anti-Black language, African American English (AAE) dialect,\nand vulgarity. Our results show strong associations between annotator identity\nand beliefs and their ratings of toxicity. Notably, more conservative\nannotators and those who scored highly on our scale for racist beliefs were\nless likely to rate anti-Black language as toxic, but more likely to rate AAE\nas toxic. We additionally present a case study illustrating how a popular\ntoxicity detection system's ratings inherently reflect only specific beliefs\nand perspectives. Our findings call for contextualizing toxicity labels in\nsocial variables, which raises immense implications for toxic language\nannotation and detection.", "collection_id": "9596620b6962dd8aee8f34781b2c1a0653872cdb31aa9658ce2f9e1776559708"}, "question": "What factors influence the perceived toxicity of language in online annotations?\n", "answer": "The perceived toxicity of language can be influenced by the annotator's identity, beliefs, and demographic background, with factors such as political leaning, racist beliefs, and cultural background playing a significant role in shaping their ratings of toxicity."}
{"document": {"content": "Neural network scaling has been critical for improving the model quality in\nmany real-world machine learning applications with vast amounts of training\ndata and compute. Although this trend of scaling is affirmed to be a sure-fire\napproach for better model quality, there are challenges on the path such as the\ncomputation cost, ease of programming, and efficient implementation on parallel\ndevices. GShard is a module composed of a set of lightweight annotation APIs\nand an extension to the XLA compiler. It provides an elegant way to express a\nwide range of parallel computation patterns with minimal changes to the\nexisting model code. GShard enabled us to scale up multilingual neural machine\ntranslation Transformer model with Sparsely-Gated Mixture-of-Experts beyond 600\nbillion parameters using automatic sharding. We demonstrate that such a giant\nmodel can efficiently be trained on 2048 TPU v3 accelerators in 4 days to\nachieve far superior quality for translation from 100 languages to English\ncompared to the prior art.", "collection_id": "369097dcf47faa4849eb0ca9f91b064d8bf2dfd5afe45c1ed510d2d9c97d670d"}, "question": "How many TPU v3 accelerators were used to train the giant multilingual neural machine translation Transformer model?\n", "answer": "The giant multilingual neural machine translation Transformer model was trained on 2048 TPU v3 accelerators."}
{"document": {"content": "Programming is a powerful and ubiquitous problem-solving tool. Developing\nsystems that can assist programmers or even generate programs independently\ncould make programming more productive and accessible, yet so far incorporating\ninnovations in AI has proven challenging. Recent large-scale language models\nhave demonstrated an impressive ability to generate code, and are now able to\ncomplete simple programming tasks. However, these models still perform poorly\nwhen evaluated on more complex, unseen problems that require problem-solving\nskills beyond simply translating instructions into code. For example,\ncompetitive programming problems which require an understanding of algorithms\nand complex natural language remain extremely challenging. To address this gap,\nwe introduce AlphaCode, a system for code generation that can create novel\nsolutions to these problems that require deeper reasoning. In simulated\nevaluations on recent programming competitions on the Codeforces platform,\nAlphaCode achieved on average a ranking of top 54.3% in competitions with more\nthan 5,000 participants. We found that three key components were critical to\nachieve good and reliable performance: (1) an extensive and clean competitive\nprogramming dataset for training and evaluation, (2) large and\nefficient-to-sample transformer-based architectures, and (3) large-scale model\nsampling to explore the search space, followed by filtering based on program\nbehavior to a small set of submissions.", "collection_id": "c75f949b07ea1991c2339011f4d40ea91cc210651fe484e5c267825a267a150f"}, "question": "What ranking did AlphaCode achieve on average in simulated evaluations on recent programming competitions on the Codeforces platform?\n", "answer": "AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants."}
{"document": {"content": "Accurate reporting of energy and carbon usage is essential for understanding\nthe potential climate impacts of machine learning research. We introduce a\nframework that makes this easier by providing a simple interface for tracking\nrealtime energy consumption and carbon emissions, as well as generating\nstandardized online appendices. Utilizing this framework, we create a\nleaderboard for energy efficient reinforcement learning algorithms to\nincentivize responsible research in this area as an example for other areas of\nmachine learning. Finally, based on case studies using our framework, we\npropose strategies for mitigation of carbon emissions and reduction of energy\nconsumption. By making accounting easier, we hope to further the sustainable\ndevelopment of machine learning experiments and spur more research into energy\nefficient algorithms.", "collection_id": "fb5713ada001fcfac2006568b629096bb6b7a29a664d64747ca3027ab42a6ecb"}, "question": "What is the main goal of the framework introduced for machine learning research?\n", "answer": "The main goal of the framework is to make accurate reporting of energy and carbon usage easier, in order to understand the potential climate impacts of machine learning research and promote sustainable development of machine learning experiments."}
{"document": {"content": "Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model's\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT's and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.", "collection_id": "c764c2344cfe3d9d8f7872ea6c3394ee134e6f25caf01af9e6e47fae657f710d"}, "question": "What is the main challenge in adapting multilingual language models to under-resourced languages?\n", "answer": "The main challenge is dealing with under-resourced languages not covered at all by the models and written in scripts unseen during pretraining, resulting in a profound performance gap between resource-rich and resource-poor target languages."}
{"document": {"content": "We introduce AndroidEnv, an open-source platform for Reinforcement Learning\n(RL) research built on top of the Android ecosystem. AndroidEnv allows RL\nagents to interact with a wide variety of apps and services commonly used by\nhumans through a universal touchscreen interface. Since agents train on a\nrealistic simulation of an Android device, they have the potential to be\ndeployed on real devices. In this report, we give an overview of the\nenvironment, highlighting the significant features it provides for research,\nand we present an empirical evaluation of some popular reinforcement learning\nagents on a set of tasks built on this platform.", "collection_id": "34512b33958bef414da79028ac3c831cb00ba3e047d95b7dea9bd1eb68789832"}, "question": "What is the name of the open-source platform for Reinforcement Learning research built on top of the Android ecosystem?\n", "answer": "AndroidEnv"}
{"document": {"content": "Recent large language models often answer factual questions correctly. But\nusers can't trust any given claim a model makes without fact-checking, because\nlanguage models can hallucinate convincing nonsense. In this work we use\nreinforcement learning from human preferences (RLHP) to train \"open-book\" QA\nmodels that generate answers whilst also citing specific evidence for their\nclaims, which aids in the appraisal of correctness. Supporting evidence is\ndrawn from multiple documents found via a search engine, or from a single\nuser-provided document. Our 280 billion parameter model, GopherCite, is able to\nproduce answers with high quality supporting evidence and abstain from\nanswering when unsure. We measure the performance of GopherCite by conducting\nhuman evaluation of answers to questions in a subset of the NaturalQuestions\nand ELI5 datasets. The model's response is found to be high-quality 80\\% of the\ntime on this Natural Questions subset, and 67\\% of the time on the ELI5 subset.\nAbstaining from the third of questions for which it is most unsure improves\nperformance to 90\\% and 80\\% respectively, approaching human baselines.\nHowever, analysis on the adversarial TruthfulQA dataset shows why citation is\nonly one part of an overall strategy for safety and trustworthiness: not all\nclaims supported by evidence are true.", "collection_id": "0451cd5d8b8092d25f921b81facf5dbcfffe6ae913674205e5904a0c977aafa7"}, "question": "What percentage of the time does GopherCite produce high-quality responses on the Natural Questions subset?\n", "answer": "GopherCite produces high-quality responses 80% of the time on the Natural Questions subset, and this performance improves to 90% when it abstains from answering the questions for which it is most unsure."}
{"document": {"content": "We propose a new family of policy gradient methods for reinforcement\nlearning, which alternate between sampling data through interaction with the\nenvironment, and optimizing a \"surrogate\" objective function using stochastic\ngradient ascent. Whereas standard policy gradient methods perform one gradient\nupdate per data sample, we propose a novel objective function that enables\nmultiple epochs of minibatch updates. The new methods, which we call proximal\npolicy optimization (PPO), have some of the benefits of trust region policy\noptimization (TRPO), but they are much simpler to implement, more general, and\nhave better sample complexity (empirically). Our experiments test PPO on a\ncollection of benchmark tasks, including simulated robotic locomotion and Atari\ngame playing, and we show that PPO outperforms other online policy gradient\nmethods, and overall strikes a favorable balance between sample complexity,\nsimplicity, and wall-time.", "collection_id": "fd17a5b9714ba1c379acbcc82877f79c997a04d54fd14e721ce574e5219232da"}, "question": "What is the name of the new family of policy gradient methods proposed for reinforcement learning?\n", "answer": "The new family of policy gradient methods is called proximal policy optimization (PPO)."}
{"document": {"content": "Pretrained Language Models (LMs) have demonstrated ability to perform\nnumerical reasoning by extrapolating from a few examples in few-shot settings.\nHowever, the extent to which this extrapolation relies on robust reasoning is\nunclear. In this paper, we investigate how well these models reason with terms\nthat are less frequent in the pretraining data. In particular, we examine the\ncorrelations between the model performance on test instances and the frequency\nof terms from those instances in the pretraining data. We measure the strength\nof this correlation for a number of GPT-based language models (pretrained on\nthe Pile dataset) on various numerical deduction tasks (e.g., arithmetic and\nunit conversion). Our results consistently demonstrate that models are more\naccurate on instances whose terms are more prevalent, in some cases above\n$70\\%$ (absolute) more accurate on the top 10\\% frequent terms in comparison to\nthe bottom 10\\%. Overall, although LMs exhibit strong performance at few-shot\nnumerical reasoning tasks, our results raise the question of how much models\nactually generalize beyond pretraining data, and we encourage researchers to\ntake the pretraining data into account when interpreting evaluation results.", "collection_id": "f0d37037f88fe0abc2a0c9165ddbb0f6989229ffabe1e2f3f1e1f4b145989fbf"}, "question": "What is the difference in accuracy between the top 10% and bottom 10% frequent terms in numerical deduction tasks for GPT-based language models?\n", "answer": "The models are above 70% (absolute) more accurate on the top 10% frequent terms in comparison to the bottom 10%."}
{"document": {"content": "Text generative models (TGMs) excel in producing text that matches the style\nof human language reasonably well. Such TGMs can be misused by adversaries,\ne.g., by automatically generating fake news and fake product reviews that can\nlook authentic and fool humans. Detectors that can distinguish text generated\nby TGM from human written text play a vital role in mitigating such misuse of\nTGMs. Recently, there has been a flurry of works from both natural language\nprocessing (NLP) and machine learning (ML) communities to build accurate\ndetectors for English. Despite the importance of this problem, there is\ncurrently no work that surveys this fast-growing literature and introduces\nnewcomers to important research challenges. In this work, we fill this void by\nproviding a critical survey and review of this literature to facilitate a\ncomprehensive understanding of this problem. We conduct an in-depth error\nanalysis of the state-of-the-art detector and discuss research directions to\nguide future work in this exciting area.", "collection_id": "d06b6957cedb79f971f69267bafac3d8a615040b0213f0ed26734599960b12f9"}, "question": "What is the main purpose of detectors that can distinguish text generated by Text Generative Models from human-written text?\n", "answer": "The main purpose of detectors that can distinguish text generated by Text Generative Models from human-written text is to mitigate the misuse of TGMs, such as automatically generating fake news and fake product reviews that can look authentic and fool humans."}
{"document": {"content": "Large transformer-based language models (LMs) trained on huge text corpora\nhave shown unparalleled generation capabilities. However, controlling\nattributes of the generated language (e.g. switching topic or sentiment) is\ndifficult without modifying the model architecture or fine-tuning on\nattribute-specific data and entailing the significant cost of retraining. We\npropose a simple alternative: the Plug and Play Language Model (PPLM) for\ncontrollable language generation, which combines a pretrained LM with one or\nmore simple attribute classifiers that guide text generation without any\nfurther training of the LM. In the canonical scenario we present, the attribute\nmodels are simple classifiers consisting of a user-specified bag of words or a\nsingle learned layer with 100,000 times fewer parameters than the LM. Sampling\nentails a forward and backward pass in which gradients from the attribute model\npush the LM's hidden activations and thus guide the generation. Model samples\ndemonstrate control over a range of topics and sentiment styles, and extensive\nautomated and human annotated evaluations show attribute alignment and fluency.\nPPLMs are flexible in that any combination of differentiable attribute models\nmay be used to steer text generation, which will allow for diverse and creative\napplications beyond the examples given in this paper.", "collection_id": "31ea01c7663e057b3da42ca2fd650d7c40a68e42c31b4844880669d565df6f87"}, "question": "How do Plug and Play Language Models (PPLMs) control attributes of generated language without retraining the model?\n\n", "answer": "PPLMs control attributes of generated language by combining a pretrained language model with one or more simple attribute classifiers that guide text generation through a forward and backward pass, where gradients from the attribute model push the language model's hidden activations, without requiring any further training of the language model."}
{"document": {"content": "The detection of offensive language in the context of a dialogue has become\nan increasingly important application of natural language processing. The\ndetection of trolls in public forums (Gal\\'an-Garc\\'ia et al., 2016), and the\ndeployment of chatbots in the public domain (Wolf et al., 2017) are two\nexamples that show the necessity of guarding against adversarially offensive\nbehavior on the part of humans. In this work, we develop a training scheme for\na model to become robust to such human attacks by an iterative build it, break\nit, fix it strategy with humans and models in the loop. In detailed experiments\nwe show this approach is considerably more robust than previous systems.\nFurther, we show that offensive language used within a conversation critically\ndepends on the dialogue context, and cannot be viewed as a single sentence\noffensive detection task as in most previous work. Our newly collected tasks\nand methods will be made open source and publicly available.", "collection_id": "b8d195faae037dd2c99413ffd53a6c6f333e8d4d9b15c0c2c97eb5fadf37e135"}, "question": "What is the main goal of the training scheme developed in the work?\n", "answer": "The main goal of the training scheme is to make a model robust to adversarially offensive behavior on the part of humans by using an iterative build it, break it, fix it strategy with humans and models in the loop."}
{"document": {"content": "Despite the recent success of multi-task learning and transfer learning for\nnatural language processing (NLP), few works have systematically studied the\neffect of scaling up the number of tasks during pre-training. Towards this\ngoal, this paper introduces ExMix (Extreme Mixture): a massive collection of\n107 supervised NLP tasks across diverse domains and task-families. Using ExMix,\nwe study the effect of multi-task pre-training at the largest scale to date,\nand analyze co-training transfer amongst common families of tasks. Through this\nanalysis, we show that manually curating an ideal set of tasks for multi-task\npre-training is not straightforward, and that multi-task scaling can vastly\nimprove models on its own. Finally, we propose ExT5: a model pre-trained using\na multi-task objective of self-supervised span denoising and supervised ExMix.\nVia extensive experiments, we show that ExT5 outperforms strong T5 baselines on\nSuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of\nExMix. ExT5 also significantly improves sample efficiency while pre-training.", "collection_id": "63eb33031ab15d4ecc99cceb0f423f4d56cf14a1ef434803c632aa4403645bfa"}, "question": "What is the name of the massive collection of supervised NLP tasks introduced in the paper?\n", "answer": "ExMix (Extreme Mixture), which consists of 107 supervised NLP tasks across diverse domains and task-families."}
{"document": {"content": "We introduce Social IQa, the first largescale benchmark for commonsense\nreasoning about social situations. Social IQa contains 38,000 multiple choice\nquestions for probing emotional and social intelligence in a variety of\neveryday situations (e.g., Q: \"Jordan wanted to tell Tracy a secret, so Jordan\nleaned towards Tracy. Why did Jordan do this?\" A: \"Make sure no one else could\nhear\"). Through crowdsourcing, we collect commonsense questions along with\ncorrect and incorrect answers about social interactions, using a new framework\nthat mitigates stylistic artifacts in incorrect answers by asking workers to\nprovide the right answer to a different but related question. Empirical results\nshow that our benchmark is challenging for existing question-answering models\nbased on pretrained language models, compared to human performance (>20% gap).\nNotably, we further establish Social IQa as a resource for transfer learning of\ncommonsense knowledge, achieving state-of-the-art performance on multiple\ncommonsense reasoning tasks (Winograd Schemas, COPA).", "collection_id": "a245db42276d36da7ddf8aaad13536a58b179f3401a09f4d3b94abdc06d51524"}, "question": "What is the name of the largescale benchmark for commonsense reasoning about social situations introduced in the context?\n", "answer": "Social IQa"}
{"document": {"content": "Most existing dialogue systems fail to respond properly to potentially unsafe\nuser utterances by either ignoring or passively agreeing with them. To address\nthis issue, we introduce ProsocialDialog, the first large-scale multi-turn\ndialogue dataset to teach conversational agents to respond to problematic\ncontent following social norms. Covering diverse unethical, problematic,\nbiased, and toxic situations, ProsocialDialog contains responses that encourage\nprosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb,\nRoTs). Created via a human-AI collaborative framework, ProsocialDialog consists\nof 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue\nsafety labels accompanied by free-form rationales.\n  With this dataset, we introduce a dialogue safety detection module, Canary,\ncapable of generating RoTs given conversational context, and a\nsocially-informed dialogue agent, Prost. Empirical results show that Prost\ngenerates more socially acceptable dialogues compared to other state-of-the-art\nlanguage and dialogue models in both in-domain and out-of-domain settings.\nAdditionally, Canary effectively guides conversational agents and off-the-shelf\nlanguage models to generate significantly more prosocial responses. Our work\nhighlights the promise and importance of creating and steering conversational\nAI to be socially responsible.", "collection_id": "333b2cfbe9a5592031ab41c64f02a7bb50742b484d98ecf18d56a1bc7c703d31"}, "question": "What is the name of the large-scale multi-turn dialogue dataset introduced to teach conversational agents to respond to problematic content following social norms?\n\n", "answer": "ProsocialDialog"}
{"document": {"content": "Potential harms of large language models can be mitigated by watermarking\nmodel output, i.e., embedding signals into generated text that are invisible to\nhumans but algorithmically detectable from a short span of tokens. We propose a\nwatermarking framework for proprietary language models. The watermark can be\nembedded with negligible impact on text quality, and can be detected using an\nefficient open-source algorithm without access to the language model API or\nparameters. The watermark works by selecting a randomized set of \"green\" tokens\nbefore a word is generated, and then softly promoting use of green tokens\nduring sampling. We propose a statistical test for detecting the watermark with\ninterpretable p-values, and derive an information-theoretic framework for\nanalyzing the sensitivity of the watermark. We test the watermark using a\nmulti-billion parameter model from the Open Pretrained Transformer (OPT)\nfamily, and discuss robustness and security.", "collection_id": "60465b4165e2a56fe89b006ea32a5191c2029871de41318e2e148707147e818a"}, "question": "How can the proposed watermarking framework for proprietary language models detect the watermark without access to the language model API or parameters?\n\n", "answer": "The proposed watermarking framework can detect the watermark using an efficient open-source algorithm that analyzes a short span of tokens generated by the model. The algorithm works by identifying the \"green\" tokens that were softly promoted during sampling, allowing it to detect the presence of the watermark without needing access to the language model's API or parameters."}
{"document": {"content": "This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.", "collection_id": "70620d62bcbfe7462b1edde7fa95b81e6d04e33b34d3c61deef15c27063000c0"}, "question": "What is the primary objective that allows Augmented Language Models to learn and perform various tasks?\n", "answer": "The primary objective is the missing token prediction objective, which enables Augmented Language Models to learn to reason, use tools, and act, while still performing standard natural language tasks."}
{"document": {"content": "Most existing sequence generation models produce outputs in one pass, usually\nleft-to-right. However, this is in contrast with a more natural approach that\nhumans use in generating content; iterative refinement and editing. Recent work\nhas introduced edit-based models for various tasks (such as neural machine\ntranslation and text style transfer), but these generally model a single edit\nstep. In this work, we propose modeling editing processes, modeling the whole\nprocess of iteratively generating sequences. We form a conceptual framework to\ndescribe the likelihood of multi-step edits, and describe neural models that\ncan learn a generative model of sequences based on these multistep edits. We\nintroduce baseline results and metrics on this task, finding that modeling\nediting processes improves performance on a variety of axes on both our\nproposed task and related downstream tasks compared to previous single-step\nmodels of edits.", "collection_id": "45a989ee5524e3d57381fc6ae1a4cc9ac6eb010d149d4d7ffeaa58bf56c21e24"}, "question": "How do most existing sequence generation models produce outputs?\n", "answer": "Most existing sequence generation models produce outputs in one pass, usually left-to-right."}
{"document": {"content": "Few-shot learning is a challenging task that requires language models to\ngeneralize from limited examples. Large language models like GPT-3 and PaLM\nhave made impressive progress in this area, but they still face difficulties in\nreasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve\ntheir reasoning skills, previous work has proposed to guide the language model\nwith prompts that elicit a series of reasoning steps before giving the final\nanswer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in\nproblem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on\nReasoning Step), a novel approach that further enhances the reasoning\ncapability of language models. DIVERSE has three main components: first, it\ngenerates diverse prompts to explore different reasoning paths for the same\nquestion; second, it uses a verifier to filter out incorrect answers based on a\nweighted voting scheme; and third, it verifies each reasoning step individually\ninstead of the whole chain. We evaluate DIVERSE on the latest language model\ncode-davinci-002 and show that it achieves new state-of-the-art results on six\nof eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).", "collection_id": "4d5807fe2b44524a71c892ceb90548c07a9cfb5cac99707b72179f8403687d02"}, "question": "What is the problem-solving rate achieved by guiding language models with prompts that elicit a series of reasoning steps on the GSM8K benchmark?\n\n", "answer": "The problem-solving rate achieved by guiding language models with prompts that elicit a series of reasoning steps on the GSM8K benchmark is 58.1%, which is a significant improvement from the original 17.9%."}
{"document": {"content": "Recent hardware developments have dramatically increased the scale of data\nparallelism available for neural network training. Among the simplest ways to\nharness next-generation hardware is to increase the batch size in standard\nmini-batch neural network training algorithms. In this work, we aim to\nexperimentally characterize the effects of increasing the batch size on\ntraining time, as measured by the number of steps necessary to reach a goal\nout-of-sample error. We study how this relationship varies with the training\nalgorithm, model, and data set, and find extremely large variation between\nworkloads. Along the way, we show that disagreements in the literature on how\nbatch size affects model quality can largely be explained by differences in\nmetaparameter tuning and compute budgets at different batch sizes. We find no\nevidence that larger batch sizes degrade out-of-sample performance. Finally, we\ndiscuss the implications of our results on efforts to train neural networks\nmuch faster in the future. Our experimental data is publicly available as a\ndatabase of 71,638,836 loss measurements taken over the course of training for\n168,160 individual models across 35 workloads.", "collection_id": "912e51f1bb8ca55a13abe589806fea6f79c7ba62bfecad72aaa8618e443e5727"}, "question": "How many loss measurements were taken over the course of training for the experimental data in the study on batch size effects on neural network training?\n\n", "answer": "The experimental data in the study consists of a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads."}
{"document": {"content": "Learning to autonomously navigate the web is a difficult sequential decision\nmaking task. The state and action spaces are large and combinatorial in nature,\nand websites are dynamic environments consisting of several pages. One of the\nbottlenecks of training web navigation agents is providing a learnable\ncurriculum of training environments that can cover the large variety of\nreal-world websites. Therefore, we propose using Adversarial Environment\nGeneration (AEG) to generate challenging web environments in which to train\nreinforcement learning (RL) agents. We provide a new benchmarking environment,\ngMiniWoB, which enables an RL adversary to use compositional primitives to\nlearn to generate arbitrarily complex websites. To train the adversary, we\npropose a new technique for maximizing regret using the difference in the\nscores obtained by a pair of navigator agents. Our results show that our\napproach significantly outperforms prior methods for minimax regret AEG. The\nregret objective trains the adversary to design a curriculum of environments\nthat are \"just-the-right-challenge\" for the navigator agents; our results show\nthat over time, the adversary learns to generate increasingly complex web\nnavigation tasks. The navigator agents trained with our technique learn to\ncomplete challenging, high-dimensional web navigation tasks, such as form\nfilling, booking a flight etc. We show that the navigator agent trained with\nour proposed Flexible b-PAIRED technique significantly outperforms competitive\nautomatic curriculum generation baselines -- including a state-of-the-art RL\nweb navigation approach -- on a set of challenging unseen test environments,\nand achieves more than 80% success rate on some tasks.", "collection_id": "f8975a787cce47bfd635662c0f939ed922177f482287186c72d4fd627c6d2070"}, "question": "What is the name of the new benchmarking environment proposed for training reinforcement learning agents in web navigation tasks?\n\n", "answer": "The new benchmarking environment is called gMiniWoB, which enables an RL adversary to use compositional primitives to learn to generate arbitrarily complex websites."}
{"document": {"content": "Despite recent progress, state-of-the-art question answering models remain\nvulnerable to a variety of adversarial attacks. While dynamic adversarial data\ncollection, in which a human annotator tries to write examples that fool a\nmodel-in-the-loop, can improve model robustness, this process is expensive\nwhich limits the scale of the collected data. In this work, we are the first to\nuse synthetic adversarial data generation to make question answering models\nmore robust to human adversaries. We develop a data generation pipeline that\nselects source passages, identifies candidate answers, generates questions,\nthen finally filters or re-labels them to improve quality. Using this approach,\nwe amplify a smaller human-written adversarial dataset to a much larger set of\nsynthetic question-answer pairs. By incorporating our synthetic data, we\nimprove the state-of-the-art on the AdversarialQA dataset by 3.7F1 and improve\nmodel generalisation on nine of the twelve MRQA datasets. We further conduct a\nnovel human-in-the-loop evaluation to show that our models are considerably\nmore robust to new human-written adversarial examples: crowdworkers can fool\nour model only 8.8% of the time on average, compared to 17.6% for a model\ntrained without synthetic data.", "collection_id": "beae55168cdd47a57b10983766de4d87a9ca0c0d9729c93efbdabaa9a409d87f"}, "question": "How much did the state-of-the-art on the AdversarialQA dataset improve after incorporating synthetic data?\n", "answer": "The state-of-the-art on the AdversarialQA dataset improved by 3.7F1 after incorporating synthetic data."}
{"document": {"content": "How can an end-user provide feedback if a deployed structured prediction\nmodel generates inconsistent output, ignoring the structural complexity of\nhuman language? This is an emerging topic with recent progress in synthetic or\nconstrained settings, and the next big leap would require testing and tuning\nmodels in real-world settings. We present a new dataset, Interscript,\ncontaining user feedback on a deployed model that generates complex everyday\ntasks. Interscript contains 8,466 data points -- the input is a possibly\nerroneous script and a user feedback, and the output is a modified script. We\nposit two use-cases of \\ours that might significantly advance the\nstate-of-the-art in interactive learning. The dataset is available at:\nhttps://github.com/allenai/interscript.", "collection_id": "349b3b49bb37d1cda23514642c9b4ab7b3c4c5b5cd71b85679a6624277cb9855"}, "question": "Where can I find the Interscript dataset for testing and tuning models in real-world settings?\n", "answer": "The Interscript dataset is available at https://github.com/allenai/interscript."}
{"document": {"content": "We introduce a large scale MAchine Reading COmprehension dataset, which we\nname MS MARCO. The dataset comprises of 1,010,916 anonymized\nquestions---sampled from Bing's search query logs---each with a human generated\nanswer and 182,669 completely human rewritten generated answers. In addition,\nthe dataset contains 8,841,823 passages---extracted from 3,563,535 web\ndocuments retrieved by Bing---that provide the information necessary for\ncurating the natural language answers. A question in the MS MARCO dataset may\nhave multiple answers or no answers at all. Using this dataset, we propose\nthree different tasks with varying levels of difficulty: (i) predict if a\nquestion is answerable given a set of context passages, and extract and\nsynthesize the answer as a human would (ii) generate a well-formed answer (if\npossible) based on the context passages that can be understood with the\nquestion and passage context, and finally (iii) rank a set of retrieved\npassages given a question. The size of the dataset and the fact that the\nquestions are derived from real user search queries distinguishes MS MARCO from\nother well-known publicly available datasets for machine reading comprehension\nand question-answering. We believe that the scale and the real-world nature of\nthis dataset makes it attractive for benchmarking machine reading comprehension\nand question-answering models.", "collection_id": "b76cb2fd2916a00de14bd10d234c42f3b72a1c396e5ffcb0c1789ad11e513bb7"}, "question": "How many anonymized questions are included in the MS MARCO dataset?\n", "answer": "The MS MARCO dataset comprises 1,010,916 anonymized questions, which were sampled from Bing's search query logs."}
{"document": {"content": "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language\nmodel trained on the Pile, whose weights will be made freely and openly\navailable to the public through a permissive license. It is, to the best of our\nknowledge, the largest dense autoregressive model that has publicly available\nweights at the time of submission. In this work, we describe \\model{}'s\narchitecture and training and evaluate its performance on a range of\nlanguage-understanding, mathematics, and knowledge-based tasks. We find that\nGPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in\nperformance when evaluated five-shot than similarly sized GPT-3 and FairSeq\nmodels. We open-source the training and evaluation code, as well as the model\nweights, at https://github.com/EleutherAI/gpt-neox.", "collection_id": "4de684298037ec53dc47fa5db1cb162d9c24ac1d7f10a031ccda27074e2b3843"}, "question": "What is the name of the 20 billion parameter autoregressive language model introduced by EleutherAI?\n", "answer": "GPT-NeoX-20B"}
{"document": {"content": "Technology for language generation has advanced rapidly, spurred by\nadvancements in pre-training large models on massive amounts of data and the\nneed for intelligent agents to communicate in a natural manner. While\ntechniques can effectively generate fluent text, they can also produce\nundesirable societal biases that can have a disproportionately negative impact\non marginalized populations. Language generation presents unique challenges for\nbiases in terms of direct user interaction and the structure of decoding\ntechniques. To better understand these challenges, we present a survey on\nsocietal biases in language generation, focusing on how data and techniques\ncontribute to biases and progress towards reducing biases. Motivated by a lack\nof studies on biases from decoding techniques, we also conduct experiments to\nquantify the effects of these techniques. By further discussing general trends\nand open challenges, we call to attention promising directions for research and\nthe importance of fairness and inclusivity considerations for language\ngeneration applications.", "collection_id": "7ef168d4bdd2ccb3a53ff2f157e35f57362220346ff198c7823ddd2fc241c8da"}, "question": "What is a major challenge in language generation technology that can have a disproportionately negative impact on marginalized populations?\n", "answer": "Societal biases, which can be produced by language generation techniques and are a major challenge in the field, particularly in terms of direct user interaction and the structure of decoding techniques."}
{"document": {"content": "We present a new question set, text corpus, and baselines assembled to\nencourage AI research in advanced question answering. Together, these\nconstitute the AI2 Reasoning Challenge (ARC), which requires far more powerful\nknowledge and reasoning than previous challenges such as SQuAD or SNLI. The ARC\nquestion set is partitioned into a Challenge Set and an Easy Set, where the\nChallenge Set contains only questions answered incorrectly by both a\nretrieval-based algorithm and a word co-occurence algorithm. The dataset\ncontains only natural, grade-school science questions (authored for human\ntests), and is the largest public-domain set of this kind (7,787 questions). We\ntest several baselines on the Challenge Set, including leading neural models\nfrom the SQuAD and SNLI tasks, and find that none are able to significantly\noutperform a random baseline, reflecting the difficult nature of this task. We\nare also releasing the ARC Corpus, a corpus of 14M science sentences relevant\nto the task, and implementations of the three neural baseline models tested.\nCan your model perform better? We pose ARC as a challenge to the community.", "collection_id": "28741b8e50eb2eff54d63a3eaad24e07973c775e0bfb0b7b6bb0dd26f06cff92"}, "question": "How many questions are in the AI2 Reasoning Challenge (ARC) dataset?\n", "answer": "The ARC dataset contains 7,787 natural, grade-school science questions."}
{"document": {"content": "By conditioning on natural language instructions, large language models\n(LLMs) have displayed impressive capabilities as general-purpose computers.\nHowever, task performance depends significantly on the quality of the prompt\nused to steer the model, and most effective prompts have been handcrafted by\nhumans. Inspired by classical program synthesis and the human approach to\nprompt engineering, we propose Automatic Prompt Engineer (APE) for automatic\ninstruction generation and selection. In our method, we treat the instruction\nas the \"program,\" optimized by searching over a pool of instruction candidates\nproposed by an LLM in order to maximize a chosen score function. To evaluate\nthe quality of the selected instruction, we evaluate the zero-shot performance\nof another LLM following the selected instruction. Experiments on 24 NLP tasks\nshow that our automatically generated instructions outperform the prior LLM\nbaseline by a large margin and achieve better or comparable performance to the\ninstructions generated by human annotators on 19/24 tasks. We conduct extensive\nqualitative and quantitative analyses to explore the performance of APE. We\nshow that APE-engineered prompts can be applied to steer models toward\ntruthfulness and/or informativeness, as well as to improve few-shot learning\nperformance by simply prepending them to standard in-context learning prompts.\nPlease check out our webpage at\nhttps://sites.google.com/view/automatic-prompt-engineer.", "collection_id": "0eb06cbb1c41913c90a674c12fea939597dad870ac651d6fc94ea33e4984a4dd"}, "question": "How many NLP tasks were used to evaluate the performance of the Automatic Prompt Engineer (APE) method?\n\n", "answer": "The Automatic Prompt Engineer (APE) method was evaluated on 24 NLP tasks, showing that automatically generated instructions outperformed the prior LLM baseline by a large margin and achieved better or comparable performance to human-generated instructions on 19 of those tasks."}
{"document": {"content": "The increasing fluency and widespread usage of large language models (LLMs)\nhighlight the desirability of corresponding tools aiding detection of\nLLM-generated text. In this paper, we identify a property of the structure of\nan LLM's probability function that is useful for such detection. Specifically,\nwe demonstrate that text sampled from an LLM tends to occupy negative curvature\nregions of the model's log probability function. Leveraging this observation,\nwe then define a new curvature-based criterion for judging if a passage is\ngenerated from a given LLM. This approach, which we call DetectGPT, does not\nrequire training a separate classifier, collecting a dataset of real or\ngenerated passages, or explicitly watermarking generated text. It uses only log\nprobabilities computed by the model of interest and random perturbations of the\npassage from another generic pre-trained language model (e.g., T5). We find\nDetectGPT is more discriminative than existing zero-shot methods for model\nsample detection, notably improving detection of fake news articles generated\nby 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline\nto 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,\ndata, and other project information.", "collection_id": "7e6d4455429a5380ccc1678ca0874d5b2590e040059d3e5d765281eb543c5c61"}, "question": "What is the name of the approach that uses a curvature-based criterion to detect if a passage is generated from a given large language model?\n", "answer": "The approach is called DetectGPT."}
{"document": {"content": "Machine learning models trained on confidential datasets are increasingly\nbeing deployed for profit. Machine Learning as a Service (MLaaS) has made such\nmodels easily accessible to end-users. Prior work has developed model\nextraction attacks, in which an adversary extracts an approximation of MLaaS\nmodels by making black-box queries to it. However, none of these works is able\nto satisfy all the three essential criteria for practical model extraction: (1)\nthe ability to work on deep learning models, (2) the non-requirement of domain\nknowledge and (3) the ability to work with a limited query budget. We design a\nmodel extraction framework that makes use of active learning and large public\ndatasets to satisfy them. We demonstrate that it is possible to use this\nframework to steal deep classifiers trained on a variety of datasets from image\nand text domains. By querying a model via black-box access for its top\nprediction, our framework improves performance on an average over a uniform\nnoise baseline by 4.70x for image tasks and 2.11x for text tasks respectively,\nwhile using only 30% (30,000 samples) of the public dataset at its disposal.", "collection_id": "79c592d597fa39e5139915e80a589464416b7b5a8469daab7258e1bd48f631ef"}, "question": "What is the average improvement in performance of the model extraction framework over a uniform noise baseline for image tasks?\n\n", "answer": "The model extraction framework improves performance on an average over a uniform noise baseline by 4.70x for image tasks."}
{"document": {"content": "The intriguing phenomenon of adversarial examples has attracted significant\nattention in machine learning and what might be more surprising to the\ncommunity is the existence of universal adversarial perturbations (UAPs), i.e.\na single perturbation to fool the target DNN for most images. With the focus on\nUAP against deep classifiers, this survey summarizes the recent progress on\nuniversal adversarial attacks, discussing the challenges from both the attack\nand defense sides, as well as the reason for the existence of UAP. We aim to\nextend this work as a dynamic survey that will regularly update its content to\nfollow new works regarding UAP or universal attack in a wide range of domains,\nsuch as image, audio, video, text, etc. Relevant updates will be discussed at:\nhttps://bit.ly/2SbQlLG. We welcome authors of future works in this field to\ncontact us for including your new finding.", "collection_id": "61d67d3ec066bae0d2e5051c9898a64fded7bdee877fef238845fad3c17c6a7a"}, "question": "What is the purpose of the survey on universal adversarial perturbations?\n", "answer": "The purpose of the survey is to summarize the recent progress on universal adversarial attacks, discuss the challenges from both the attack and defense sides, and explore the reason for the existence of universal adversarial perturbations, with the aim of regularly updating its content to follow new works in this field."}
{"document": {"content": "Extractive reading comprehension systems can often locate the correct answer\nto a question in a context document, but they also tend to make unreliable\nguesses on questions for which the correct answer is not stated in the context.\nExisting datasets either focus exclusively on answerable questions, or use\nautomatically generated unanswerable questions that are easy to identify. To\naddress these weaknesses, we present SQuAD 2.0, the latest version of the\nStanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD\ndata with over 50,000 unanswerable questions written adversarially by\ncrowdworkers to look similar to answerable ones. To do well on SQuAD 2.0,\nsystems must not only answer questions when possible, but also determine when\nno answer is supported by the paragraph and abstain from answering. SQuAD 2.0\nis a challenging natural language understanding task for existing models: a\nstrong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on\nSQuAD 2.0.", "collection_id": "cdfbd40358e2b13bf3c3c833a9af9b3e625065e93cb8ed9b61d48255409edadc"}, "question": "What is the name of the latest version of the Stanford Question Answering Dataset that combines existing data with unanswerable questions?\n", "answer": "SQuAD 2.0"}
{"document": {"content": "We present the results and main findings of SemEval-2020 Task 12 on\nMultilingual Offensive Language Identification in Social Media (OffensEval\n2020). The task involves three subtasks corresponding to the hierarchical\ntaxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The\ntask featured five languages: English, Arabic, Danish, Greek, and Turkish for\nSubtask A. In addition, English also featured Subtasks B and C. OffensEval 2020\nwas one of the most popular tasks at SemEval-2020 attracting a large number of\nparticipants across all subtasks and also across all languages. A total of 528\nteams signed up to participate in the task, 145 teams submitted systems during\nthe evaluation period, and 70 submitted system description papers.", "collection_id": "5fd2a26318e318ee64ecdff2361306611f0678346b46cad6709945b21f7a6616"}, "question": "How many teams signed up to participate in OffensEval 2020?\n", "answer": "A total of 528 teams signed up to participate in OffensEval 2020."}
{"document": {"content": "Dialogue systems in the form of chatbots and personal assistants are being\nincreasingly integrated into people's lives. Modern dialogue systems may\nconsider adopting anthropomorphic personas, mimicking societal demographic\ngroups to appear more approachable and trustworthy to users. However, the\nadoption of a persona can result in the adoption of biases. In this paper, we\npresent the first large-scale study on persona biases in dialogue systems and\nconduct analyses on personas of different social classes, sexual orientations,\nraces, and genders. We define persona biases as harmful differences in\nresponses (e.g., varying levels of offensiveness, agreement with harmful\nstatements) generated from adopting different demographic personas.\nFurthermore, we introduce an open-source framework, UnitPersonaBias, to explore\nand aggregate persona biases in dialogue systems. By analyzing the Blender and\nDialoGPT dialogue systems, we observe that adopting personas can actually\ndecrease harmful responses, compared to not using any personas. Additionally,\nwe find that persona choices can affect the degree of harms in generated\nresponses and thus should be systematically evaluated before deployment. We\nalso analyze how personas can result in different amounts of harm towards\nspecific demographics.", "collection_id": "24ca58154695e9edcf4641c14923c1c131cc046928b9e651d957ef783bbc2e32"}, "question": "What is the name of the open-source framework introduced to explore and aggregate persona biases in dialogue systems?\n\n", "answer": "UnitPersonaBias"}
{"document": {"content": "We present Chirpy Cardinal, an open-domain dialogue agent, as a research\nplatform for the 2019 Alexa Prize competition. Building an open-domain\nsocialbot that talks to real people is challenging - such a system must meet\nmultiple user expectations such as broad world knowledge, conversational style,\nand emotional connection. Our socialbot engages users on their terms -\nprioritizing their interests, feelings and autonomy. As a result, our socialbot\nprovides a responsive, personalized user experience, capable of talking\nknowledgeably about a wide variety of topics, as well as chatting\nempathetically about ordinary life. Neural generation plays a key role in\nachieving these goals, providing the backbone for our conversational and\nemotional tone. At the end of the competition, Chirpy Cardinal progressed to\nthe finals with an average rating of 3.6/5.0, a median conversation duration of\n2 minutes 16 seconds, and a 90th percentile duration of over 12 minutes.", "collection_id": "0b03bdd2ba4e42eb7f74d316cad840abceb79211dbc87898313dc3bad1ba967a"}, "question": "What was the average rating of Chirpy Cardinal at the end of the 2019 Alexa Prize competition?\n", "answer": "Chirpy Cardinal received an average rating of 3.6 out of 5.0 at the end of the competition."}
{"document": {"content": "Recent progress in pre-trained neural language models has significantly\nimproved the performance of many natural language processing (NLP) tasks. In\nthis paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT\nwith disentangled attention) that improves the BERT and RoBERTa models using\ntwo novel techniques. The first is the disentangled attention mechanism, where\neach word is represented using two vectors that encode its content and\nposition, respectively, and the attention weights among words are computed\nusing disentangled matrices on their contents and relative positions,\nrespectively. Second, an enhanced mask decoder is used to incorporate absolute\npositions in the decoding layer to predict the masked tokens in model\npre-training. In addition, a new virtual adversarial training method is used\nfor fine-tuning to improve models' generalization. We show that these\ntechniques significantly improve the efficiency of model pre-training and the\nperformance of both natural language understanding (NLU) and natural langauge\ngeneration (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model\ntrained on half of the training data performs consistently better on a wide\nrange of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),\non SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).\nNotably, we scale up DeBERTa by training a larger version that consists of 48\nTransform layers with 1.5 billion parameters. The significant performance boost\nmakes the single DeBERTa model surpass the human performance on the SuperGLUE\nbenchmark (Wang et al., 2019a) for the first time in terms of macro-average\nscore (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the\nSuperGLUE leaderboard as of January 6, 2021, out performing the human baseline\nby a decent margin (90.3 versus 89.8).", "collection_id": "8e22c4c67b8211fca748895ec8a37e9223092616c696216127559ecbd758825f"}, "question": "What is the name of the new model architecture proposed in the paper that improves the BERT and RoBERTa models?\n\n", "answer": "DeBERTa (Decoding-enhanced BERT with disentangled attention)"}
{"document": {"content": "Existing pre-trained models are generally geared towards a particular class\nof problems. To date, there seems to be still no consensus on what the right\narchitecture and pre-training setup should be. This paper presents a unified\nframework for pre-training models that are universally effective across\ndatasets and setups. We begin by disentangling architectural archetypes with\npre-training objectives -- two concepts that are commonly conflated. Next, we\npresent a generalized & unified perspective for self-supervision in NLP and\nshow how different pre-training objectives can be cast as one another and how\ninterpolating between different objectives can be effective. We then propose\nMixture-of-Denoisers (MoD), a pre-training objective that combines diverse\npre-training paradigms together. We furthermore introduce a notion of mode\nswitching, wherein downstream fine-tuning is associated with specific\npre-training schemes. We conduct extensive ablative experiments to compare\nmultiple pre-training objectives and find that our method pushes the\nPareto-frontier by outperforming T5 & GPT-like models across multiple diverse\nsetups. By scaling our model up to 20B parameters, we achieve SOTA performance\non 50 well-established supervised finetuning based NLP tasks. Our model also\nachieve strong results at in-context learning, outperforming 175B GPT-3 on\nzero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot\nsummarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B\nalso works well with chain-of-thought prompting and reasoning, making it an\nappealing choice for research into reasoning at a small to medium scale of 20B\nparameters. Finally, we apply FLAN instruction tuning to the UL2 20B model,\nachieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release\nFlax-based T5X checkpoints for the UL2 20B & Flan-UL2 20B.", "collection_id": "e880c739f1c9cc772624123d7fbec01816475cc751c7e648a7862dd572be7e98"}, "question": "What is the name of the pre-training objective proposed in the paper that combines diverse pre-training paradigms together?\n\n", "answer": "The pre-training objective proposed in the paper is called Mixture-of-Denoisers (MoD)."}
{"document": {"content": "We show that in a variety of large-scale deep learning scenarios the gradient\ndynamically converges to a very small subspace after a short period of\ntraining. The subspace is spanned by a few top eigenvectors of the Hessian\n(equal to the number of classes in the dataset), and is mostly preserved over\nlong periods of training. A simple argument then suggests that gradient descent\nmay happen mostly in this subspace. We give an example of this effect in a\nsolvable model of classification, and we comment on possible implications for\noptimization and learning.", "collection_id": "42db42ca57794a74125770c568f65072be821b7e7a5dfbffe67a1319ab989722"}, "question": "What is the dimensionality of the subspace that the gradient dynamically converges to in large-scale deep learning scenarios?\n", "answer": "The subspace is spanned by a few top eigenvectors of the Hessian, equal to the number of classes in the dataset."}
{"document": {"content": "Position encoding recently has shown effective in the transformer\narchitecture. It enables valuable supervision for dependency modeling between\nelements at different positions of the sequence. In this paper, we first\ninvestigate various methods to integrate positional information into the\nlearning process of transformer-based language models. Then, we propose a novel\nmethod named Rotary Position Embedding(RoPE) to effectively leverage the\npositional information. Specifically, the proposed RoPE encodes the absolute\nposition with a rotation matrix and meanwhile incorporates the explicit\nrelative position dependency in self-attention formulation. Notably, RoPE\nenables valuable properties, including the flexibility of sequence length,\ndecaying inter-token dependency with increasing relative distances, and the\ncapability of equipping the linear self-attention with relative position\nencoding. Finally, we evaluate the enhanced transformer with rotary position\nembedding, also called RoFormer, on various long text classification benchmark\ndatasets. Our experiments show that it consistently overcomes its alternatives.\nFurthermore, we provide a theoretical analysis to explain some experimental\nresults. RoFormer is already integrated into Huggingface:\n\\url{https://huggingface.co/docs/transformers/model_doc/roformer}.", "collection_id": "05a3e9e12607e8d44ec37fd598ab53578ac001ea7022031a39d5c126db5d3135"}, "question": "What is the name of the novel method proposed to effectively leverage positional information in transformer-based language models?\n", "answer": "The novel method is named Rotary Position Embedding (RoPE)."}
{"document": {"content": "We describe our early efforts to red team language models in order to\nsimultaneously discover, measure, and attempt to reduce their potentially\nharmful outputs. We make three main contributions. First, we investigate\nscaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B\nparameters) and 4 model types: a plain language model (LM); an LM prompted to\nbe helpful, honest, and harmless; an LM with rejection sampling; and a model\ntrained to be helpful and harmless using reinforcement learning from human\nfeedback (RLHF). We find that the RLHF models are increasingly difficult to red\nteam as they scale, and we find a flat trend with scale for the other model\ntypes. Second, we release our dataset of 38,961 red team attacks for others to\nanalyze and learn from. We provide our own analysis of the data and find a\nvariety of harmful outputs, which range from offensive language to more subtly\nharmful non-violent unethical outputs. Third, we exhaustively describe our\ninstructions, processes, statistical methodologies, and uncertainty about red\nteaming. We hope that this transparency accelerates our ability to work\ntogether as a community in order to develop shared norms, practices, and\ntechnical standards for how to red team language models.", "collection_id": "ffe9d71410874f7d0de05b52a63e2f3c87afce1e2a1236828e1cbcae5d51cd74"}, "question": "What is the size range of the language models investigated in the red teaming efforts?\n", "answer": "The language models investigated in the red teaming efforts range in size from 2.7 billion parameters to 52 billion parameters."}
{"document": {"content": "BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that\nfocuses on tasks believed to be beyond the capabilities of current language\nmodels. Language models have already made good progress on this benchmark, with\nthe best model in the BIG-Bench paper outperforming average reported\nhuman-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But\non what tasks do language models fall short of average human-rater performance,\nand are those tasks actually unsolvable by current language models?\n  In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we\ncall BIG-Bench Hard (BBH). These are the task for which prior language model\nevaluations did not outperform the average human-rater. We find that applying\nchain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the\naverage human-rater performance on 10 of the 23 tasks, and Codex\n(code-davinci-002) to surpass the average human-rater performance on 17 of the\n23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot\nprompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al.,\n2022), substantially underestimates the best performance and capabilities of\nlanguage models, which is better captured via CoT prompting. As further\nanalysis, we explore the interaction between CoT and model scale on BBH,\nfinding that CoT enables emergent task performance on several BBH tasks with\notherwise flat scaling curves.", "collection_id": "abe8a86a3b45f5e1d22ff7f4685cae6abb23fb99deae068ca15ad897e39d7d5c"}, "question": "What percentage of BIG-Bench tasks did the best model in the BIG-Bench paper outperform average reported human-rater results on via few-shot prompting?\n\n", "answer": "The best model in the BIG-Bench paper outperformed average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting."}
